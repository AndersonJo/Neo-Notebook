{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "* Q Table은 초기에 0값 (np.zeros) 로 시작합니다. \n",
    "* Q Table의 row는 states가 되고, column은 actions로 간주\n",
    "\n",
    "### ForzenLake v0\n",
    "\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "Actions \n",
    "\n",
    "* 오른쪽으로 이동: 3\n",
    "\n",
    "\n",
    "### Bellman Equation \n",
    "\n",
    "\n",
    "\n",
    "## $$ Q(s, a) = r + \\gamma( max(Q(s^{\\prime}, a^{\\prime}))) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation\n",
    "\n",
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "Total Score: 0.0 Move Count: 18\n"
     ]
    }
   ],
   "source": [
    "class QLearning(object):\n",
    "    \n",
    "    def __init__(self, game, epoch = 20000, lr=0.85, gamma=0.90):\n",
    "        self.env = gym.make(game)\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.Q = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        \n",
    "    def train(self):\n",
    "        env, Q, lr, gamma = self.env, self.Q, self.lr, self.gamma\n",
    "        for i in xrange(1, self.epoch+1):\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            while True:\n",
    "                action = np.argmax(Q[state,:] + np.random.randn(1, env.action_space.n)*(1./(i+1)))\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                Q[state, action] += lr*(reward + gamma * max(Q[next_state,:]) - Q[state, action])\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "                state = next_state\n",
    "            \n",
    "            if i% 1000 == 0:\n",
    "                self.render()\n",
    "                print 'epoch:', i\n",
    "                print Q\n",
    "            \n",
    "    def play(self):\n",
    "        env = self.env\n",
    "        score = 0\n",
    "        count = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) * 0.01)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            \n",
    "            self.render()\n",
    "            count += 1\n",
    "            score += reward\n",
    "            if done:\n",
    "                self.render()\n",
    "                break\n",
    "        print 'Total Score:', score, 'Move Count:', count\n",
    "    \n",
    "    def render(self):\n",
    "        clear_output(True)\n",
    "        self.env.render()\n",
    "        sleep(0.1)\n",
    "\n",
    "q = QLearning('FrozenLake-v0')\n",
    "q.train()\n",
    "q.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-19 22:43:54,665] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.88759455e-01   6.54992123e-03   8.41997691e-03   5.37595561e-03]\n",
      " [  7.16261270e-03   1.18899485e-03   5.84362665e-03   5.15822614e-01]\n",
      " [  6.04489565e-03   4.55799800e-03   1.21974078e-03   6.54431115e-01]\n",
      " [  6.51110492e-04   4.13315422e-03   4.46765414e-03   3.89867052e-01]\n",
      " [  8.78078560e-01   1.19218779e-03   1.12876108e-03   1.33154362e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.27943330e-03   1.57400011e-05   8.31065691e-02   2.47979630e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.13719353e-03   7.91424504e-04   1.48208782e-04   9.55378413e-01]\n",
      " [  3.18288318e-04   9.78880595e-01   4.16395414e-04   0.00000000e+00]\n",
      " [  9.57102244e-01   3.90364280e-04   5.96811765e-04   1.89703368e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   2.38438588e-03   9.89929312e-01   2.83349481e-03]\n",
      " [  0.00000000e+00   9.98724828e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .85\n",
    "y = .99\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "\n",
    "print Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
