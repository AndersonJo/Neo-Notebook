{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "* Q Table은 초기에 0값 (np.zeros) 로 시작합니다. \n",
    "* Q Table의 row는 states가 되고, column은 actions로 간주\n",
    "\n",
    "### ForzenLake v0\n",
    "\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "Actions \n",
    "\n",
    "* 오른쪽으로 이동: 3\n",
    "\n",
    "\n",
    "### Bellman Equation \n",
    "\n",
    "\n",
    "\n",
    "## $$ Q(s, a) = learning\\ rate \\cdot (r + \\gamma( max(Q(s^{\\prime}, a^{\\prime})))) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation\n",
    "\n",
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "  (Down)\n",
      "Total Score: 1.0 Move Count: 200\n"
     ]
    }
   ],
   "source": [
    "class QLearning(object):\n",
    "    \n",
    "    def __init__(self, game, epoch = 5000, lr=0.9, gamma=0.95):\n",
    "        self.env = gym.make(game)\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.Q = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        \n",
    "    def train(self):\n",
    "        env, Q, lr, gamma = self.env, self.Q, self.lr, self.gamma\n",
    "        for i in xrange(1, self.epoch+1):\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            while True:\n",
    "                action = np.argmax(Q[state,:] + np.random.randn(1, env.action_space.n)*(1./(i+1)))\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                Q[state, action] += lr*(reward + gamma * max(Q[next_state,:]) - Q[state, action])\n",
    "                    \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "                state = next_state\n",
    "            \n",
    "            if i% 1000 == 0:\n",
    "                self.render()\n",
    "                print 'epoch:', i\n",
    "                print Q\n",
    "    \n",
    "    def best(self):\n",
    "        self.Q = np.array([[0.4063956502764648, 0.0009376981089249563, 0.0013001605379982584, 0.001683139188452684], [0.0002212255055831433, 0.0001604213896660041, 4.624894221606768e-05, 0.5338252140186397], [0.000305501948144074, 0.000339043227170818, 0.0012841440451924998, 0.4116756725648396], [0.00010644579717852419, 0.00011026668390479513, 0.00011651802683072658, 0.08822724323795543], [0.24899353845276465, 7.3491024696467275e-06, 0.00014209625352832917, 0.001531405985072446], [0.0, 0.0, 0.0, 0.0], [8.618446690923737e-09, 1.3204910121685973e-05, 0.010329626640153092, 1.516061014835319e-07], [0.0, 0.0, 0.0, 0.0], [1.587967395020481e-05, 6.586909061922933e-05, 0.00020364402075925673, 0.34541789013621815], [8.494245587988491e-05, 0.7442249345200769, 0.0005817216908660322, 0.00010143897817857398], [0.9330780792795537, 6.388955763260169e-05, 8.476459888209333e-06, 1.034818501282543e-06], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.737434560439633, 0.00037395420533349166], [0.004943543729386356, 0.9907048052993814, 0.009063655039055965, 0.02159258165081837], [0.0, 0.0, 0.0, 0.0]])\n",
    "    \n",
    "    def play(self):\n",
    "        env, Q = self.env, self.Q\n",
    "        score = 0\n",
    "        count = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) * 0.01)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            \n",
    "            self.render()\n",
    "            count += 1\n",
    "            score += reward\n",
    "            if done:\n",
    "                self.render()\n",
    "                break\n",
    "        print 'Total Score:', score, 'Move Count:', count\n",
    "    \n",
    "    def render(self):\n",
    "        clear_output(True)\n",
    "        self.env.render()\n",
    "        sleep(0.05)\n",
    "\n",
    "q = QLearning('FrozenLake-v0')\n",
    "q.best()\n",
    "q.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
