{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "DeepMind Paper에 따르면 마지막 4개의 마지막 스크린을 84*84 싸이즈의 256 grayscale로 변환해줍니다.<br>\n",
    "즉 states로 $ 256^{4 * 84 * 84} = 10^{67970} $의 game states가 나올수 있으며 Q-table을 사용한다면 rows값으로 사용 될 것입니다.<br>\n",
    "해당 값은 현재 알려진 원자의 갯수보다도 더 많습니다. \n",
    "\n",
    "바로 이부분, Q-function을 neural network로 대체하는것이 DeepMind paper에서 주장하는 것입니다. <br>\n",
    "아래의 figure는 구글 딥마인드에서 사용한 CNN의 구조입니다.\n",
    "\n",
    "<img src=\"images/deep-q-learning-used-by-deepmind.png\" class=\"img-responsive img-rounded\">\n",
    "\n",
    "특이한점은 Pooling Layers가 없습니다. 이유는 pooling layers를 할 시, 게임속 objects들의 위치를 알 방법이 없어지게 됩니다.<br>\n",
    "따라서 Deep Reinforcement를 할때 CNN에서는 pooling layers를 제거합니다.\n",
    "\n",
    "\n",
    "### Deep Q-Learning\n",
    "\n",
    "Value function을 Deep Q-network로 나타냅니다. (weights $ \\theta $ 가 추가되고, 오른쪽은 기존 value function)\n",
    "\n",
    "###  $$ Q(s, a; \\theta) \\approx Q^{\\pi}(s, a) $$\n",
    "\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "### $$ target = y_i =  r + \\gamma \\cdot max_{a^{\\prime}} Q(s^{\\prime}, a^{\\prime} ; \\theta_{i-1}) $$\n",
    "### $$ L_i (\\theta_i) = \\frac{1}{2}[ y_i  - Q(s, a; \\theta_{i}) ]^{2} $$ \n",
    "\n",
    "Q-Learning에서 q-table update는 다음과 같이 변경이 됩니다. \n",
    "\n",
    "1. predicted q-values (뉴럴넷을 통해 나온 actions 값들)를 현재의 state s 로부터 feedforward pass를 통해서 알아냅니다. \n",
    "2. 다음 state $ s^{\\prime} $ 대해서 feedforward pass를 하고, network outputs에서 가장 큰값 $ max_{a^{\\prime}} Q(s^{\\prime}, a^{\\prime}) $ 을 계산합니다. \n",
    "3. action에 대한 Q-value target을 $ r  + \\gamma \\cdot max_{a^{\\prime}} Q(s^{\\prime}, a^{\\prime}) $\n",
    "4. Set Q-value target for action to r + γmax a’ Q(s’, a’) (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.\n",
    "5. Backpropagation을 통해서 update를 합니다.\n",
    "\n",
    "\n",
    "### Defferntiating the loss function\n",
    "\n",
    "### $$ \\nabla_{\\theta_i} L_i(\\theta_i) = \\sum[ ( r + \\gamma \\cdot max(a^{\\prime}) Q(s^{\\prime}, a^{\\prime}; \\theta_{i-1}) - Q(s, a; \\theta_i)) \\nabla_{\\theta_i} Q(s, a; \\theta_i)] $$ \n",
    "\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "전체 플레이 데이터를 저장하고, random minibatch를 통해서 트레이닝을 시킵니다. 이는 연속적인 samples들의 유사성을 없애기 때문에 local minimum에 빠지는 상황을 방지할수 있습니다. 또한 experience replay를 통해서 supervised learning처럼 할 수 있으며, debugging, testing을 좀 더 쉽게 할 수 있습니다. 가장 좋은 점은 인간한 게임 플레이를 통해서 경험을 쌓게 할 수 도 있습니다. \n",
    "\n",
    "1. Experience라는것은 일련의 $ s_t, a_t, r_t, s_{t+1}, a_{t+1}, r_{t+1} ... $ 같은 것들을 모음.\n",
    "2. Experience $ e_t = (s_t, a_t, r_t, s_{t+1} ) $ 를 every time-step때마다, data-set $ D = e_1, ... e_N $에다가 저장을 합니다.\n",
    "\n",
    "\n",
    "### ε-greedy exploration\n",
    "\n",
    "Agent가 처음에는 random하게 행동을 하지만, 시간이 지남에 따라서 점차 random보다는 가장 높은 q-value값을 선택하도록 하는 것이 좋습니다.\n",
    "딥마인드에서도 처음에는 ε 값을 1로 잡았다가.. 이 값을 0.1로 점차 줄여나갑니다. \n",
    "\n",
    "### Pseudo Code\n",
    "\n",
    "[Toronto DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "\n",
    "<img src=\"images/deep-q-learning-algorithm-pseudo-code.png\" class=\"img-responsive img-rounded\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "hdf5 not supported (please install/reinstall h5py)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 17:50:52,267] Making new env: Breakout-v0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e9abbe76f398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mreplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExperienceReplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/anderson/@python/neo-notebook/Artificial Intelligence/0701 Deep Reinforcement Learning/agent.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0msummary_placeholders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/anderson_qlearning.tensorboard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anderson/@python/neo-notebook/Artificial Intelligence/0701 Deep Reinforcement Learning/agent.pyc\u001b[0m in \u001b[0;36m_build_summaries\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'global_step'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'net_score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet_score\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mplaceholders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0massign_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaceholders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0msummary_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import environment \n",
    "import replay \n",
    "import agent\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output\n",
    "from pprint import pprint as pp\n",
    "\n",
    "reload(environment)\n",
    "reload(replay)\n",
    "reload(agent)\n",
    "        \n",
    "env = environment.Environment('Breakout-v0')\n",
    "replay = replay.ExperienceReplay(env)\n",
    "agent = agent.Agent(env, replay)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action = env.random_action()\n",
    "    screen, reward, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "env.close()\n",
    "    \n",
    "print screen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "* [ICLR - Deep Reinforcement Learning](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf)\n",
    "* [Carpedm20 - Deep-rl-tensorflow](https://github.com/carpedm20/deep-rl-tensorflow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
