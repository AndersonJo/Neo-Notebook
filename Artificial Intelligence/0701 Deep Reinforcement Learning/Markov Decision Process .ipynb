{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "\n",
    "\n",
    "Markov Decision Process는 4 tuple &lt;S, A, T, R&gt; 입니다.\n",
    "\n",
    "> **S:** a finite set of states (Agent의 위치, 점수, 공의 위치등등, Environment가 Agent한테 던져줌)\n",
    "\n",
    "> **A:** a finite set of actions (Agent가 취하는 행동 - 위, 아래, 오른쪽, 왼쪽, 점프 등등)\n",
    "\n",
    "> **T:** $ T(s, s^{\\prime}, a) = Pr(s_{t+1} = s^{\\prime} | s_t = s, a_t = a) $ \n",
    "\n",
    "> **R:** $ R(s, s^{\\prime}, a) $ 또는 $ R(s^{\\prime}, a) $ \n",
    "\n",
    "\n",
    "목표는 가장 rewards를 많이 받게 되는 Policy $ \\pi $를 찾는 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "시작은 $ V_{0}^{*} = 0 $ 과 같이 시작합니다. <small class=\"text-muted\">(i 는 첫번째 step 또는 시간을 가르킵니다. H (Horizon)이 마지막입니다. -> 0, 1, 2, 3, ... H)</small>\n",
    "\n",
    "\n",
    "$ V_{i}^{*}$ 일때 모든 states에 대해서 (모든 경우의 수) 계산을 해줍니다.<br>\n",
    "<small class=\"text-muted\">아래는 total reward를 구하는데, discounted reward 구하고자 하면 $ \\gamma^{i-1} reward_{i} $ 처럼 하면 됩니다.</small> \n",
    "\n",
    "## <span class=\"text-danger\"> $$ V_{i + 1}^{*}(s) = max(a) \\cdot \\sum T(s, a, s^{\\prime}) [ R(s, a, s^{\\prime}) + V_{i}^{*}(s^{\\prime}) ] $$ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "아래 예제는 Backward일때 입니다. 즉.. 종료지점부터 시작해서 나가는 방법..\n",
    "\n",
    "<img src=\"images/value_iteration_example001.png\" class=\"img-responsive img-rounded\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "MDP는 Linear Programming 또는 Dynamic Programming으로 해결할수 있습니다. 여기서는 Dynamic Programming 방식으로 해결을 합니다.\n",
    "\n",
    "$$ \\pi(s) = argmax_a\\{ \\sum_{s^{\\prime}}{ P_a(s, s^{\\prime}) R_a(s, s^{\\prime} + \\gamma V(s^{\\prime})) }  \\} $$\n",
    "\n",
    "$$ V(s) = \\sum_{s^{\\prime}}{ P_{\\pi(s)}(s, s^{\\prime})} ( R_{\\pi(s)}(s, s^{\\prime}) + \\gamma V(s^{\\prime})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = np.array([\n",
    "    [-0.04, None, -0.04, 1],\n",
    "    [-0.04, None, -0.04, -1],\n",
    "    [-0.04, -0.04, -0.04, -0.04],\n",
    "    [-0.04, -0.04, None, -0.04]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env):\n",
    "    V = np.zeros(env.shape)\n",
    "    print V\n",
    "    print env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[[-0.01 None -0.01 1]\n",
      " [-0.01 None -0.01 -1]\n",
      " [-0.01 -0.01 -0.01 -0.01]\n",
      " [-0.01 -0.01 None -0.01]]\n"
     ]
    }
   ],
   "source": [
    "value_iteration(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Washington - Markov Decision Processes](https://courses.cs.washington.edu/courses/cse473/11au/slides/cse473au11-mdps.pdf)\n",
    "* [Berkeley - Value Iteration Intro](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa11/slides/mdps-intro-value-iteration.pdf) \n",
    "* [UBS - Value Iteration](http://www.cs.ubc.ca/~kevinlb/teaching/cs322%20-%202009-10/Lectures/DT4.pdf) - pseudocode, 공식\n",
    "* [발표](https://piazza-resources.s3.amazonaws.com/hqpbdfmjns93u9/hrvetr7dmr96i2/9__Markov_Decision_Problems_II_Notes.pdf?AWSAccessKeyId=AKIAIEDNRLJ4AZKBW6HA&Expires=1479469936&Signature=egVS8%2FaEUwVnQ2wz%2Frc4UykbtRY%3D)\n",
    "* [Princeton - INTRODUCTION TO MARKOV DECISION PROCESSES](http://castlelab.princeton.edu/ORF569papers/Powell_ADP_2ndEdition_Chapter%203.pdf)\n",
    "* [UC Berkeley - Exact Solution Methods](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/mdps-exact-methods.pdf)\n",
    "* [Value Iteration](http://artint.info/html/ArtInt_227.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
