{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a966fdfd-fb69-4ece-9b37-01246776d230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:33:30.902388: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-08 16:33:30.930992: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-08 16:33:31.422146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6150a5a-f57c-490d-bbe5-a66e9c0a70a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63cca16e0794017a694215c2d4f5126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d499903c0a914598975b31c365925e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at yanolja/EEVE-Korean-10.8B-v1.0 and are newly initialized: ['model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.46.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.44.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.40.self_attn.rotary_emb.inv_freq', 'model.layers.41.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.43.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.45.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.42.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.47.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(40960, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=40960, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"KRAFTON/KORani-v3-13B\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        offload_folder=\"./offload\",\n",
    "        llm_int8_enable_fp32_cpu_offload=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    '/home/anderson/.cache/huggingface/hub/models--KRAFTON--KORani-v3-13B/',\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\",\n",
    "    # max_memory={0: \"1600MB\"},  # 최대 메모리 설정 적용\n",
    ")\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"balanced_low_0\",\n",
    "#     torch_dtype=torch.float16,  # Half Precision\n",
    "#     # low_cpu_mem_usage=True,\n",
    "#     offload_folder=\"./offload\",\n",
    "#     llm_int8_enable_fp32_cpu_offload=True,\n",
    "# )\n",
    "# Gradient Checkpointing 활성화\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24a2852f-d0f8-4009-8d26-be0968f3b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_balanced_device_map(model, prob: float = 0.5, output=\"./device_map.json\"):\n",
    "    import json\n",
    "\n",
    "    layer_names = model.state_dict().keys()\n",
    "    half_index = int(len(layer_names) * prob)\n",
    "\n",
    "    device_map = {}\n",
    "    for idx, layer_name in enumerate(layer_names):\n",
    "        if idx < half_index:\n",
    "            device_map[layer_name] = \"cuda:0\"\n",
    "        else:\n",
    "            device_map[layer_name] = \"cpu\"\n",
    "    with open(output, \"wt\") as f:\n",
    "        json.dump(device_map, f)\n",
    "\n",
    "\n",
    "calculate_balanced_device_map(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c34cc5c-12cd-4136-b969-54e33131033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_size: 756\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[    1, 28705,    13, 36958, 32877, 30275, 32218, 33442, 28705, 39959,\n",
      "         29511, 32194, 28723,    13,  1177, 13918,    13, 29828, 32809, 32717,\n",
      "         32974, 28705, 28750, 28734, 28740, 28781, 31479, 28702, 32531, 29470,\n",
      "         40055, 29844, 28705, 28750, 28734, 28750, 28750, 31479, 28702, 32531,\n",
      "         29470, 40055, 32005, 32014, 31091, 40239, 31409, 29695, 32931, 32353,\n",
      "         32850, 37893, 29189, 32002, 39979, 31718, 32531, 29470, 40055, 32110,\n",
      "         31673, 29148, 33255, 32298, 32097, 28725, 28705, 28750, 28734, 28740,\n",
      "         28781, 31479, 32089, 32014, 39961, 29833, 32232, 32564, 32459, 30403,\n",
      "         32025, 28723, 28705, 28750, 28734, 28740, 28781, 31479, 32008, 33563,\n",
      "         32297, 32809, 28705, 28781, 29653, 29164, 33693, 34046, 32046, 31073,\n",
      "         32120, 32153, 35924, 33096, 32267, 32041, 28705, 39995, 29433, 31876,\n",
      "         32178, 33518, 32026, 32352, 32014, 31091, 40239, 31409, 29695, 32931,\n",
      "         32353, 32850, 37893, 29189, 32403, 29687, 31673, 32004, 32002, 39979,\n",
      "         33190, 28705, 28787, 29653, 29164, 28705, 28781, 37910, 28705, 28740,\n",
      "         29599, 33438, 32391, 32228, 28705, 28750, 28734, 28740, 28781, 28702,\n",
      "         32531, 29470, 40055, 28705, 37925, 30589, 28705, 28770, 33197, 32046,\n",
      "         33205, 32531, 29470, 40055, 32091, 39536, 32471, 29578, 29324, 32674,\n",
      "         30615, 37903, 29189, 33142, 32025, 28723, 32356, 28705, 28750, 28734,\n",
      "         28750, 28750, 31479, 32008, 29884, 32182, 32327, 38107, 31836, 28705,\n",
      "         40165, 31306, 30856, 32117, 31495, 32703, 28705, 40347, 29511, 32072,\n",
      "         31673, 29189, 32987, 32298, 32097, 28725, 32496, 29578, 32091, 30395,\n",
      "         29143, 32674, 30615, 37903, 29189, 28705, 28750, 29884, 33142, 29282,\n",
      "         32471, 29135, 32370, 28723,    13, 29288, 29647, 40248, 32297, 33445,\n",
      "         32391, 32863, 26837, 28705, 40165, 31306, 38757, 29288, 29614, 28705,\n",
      "         28781, 31479, 33693, 28705, 37925, 30589, 37864, 28725, 26837, 28705,\n",
      "         40165, 31306, 38757, 29288, 29614, 32090, 30336, 28705, 37925, 30589,\n",
      "         37864, 28705, 28782, 29884, 28732, 30310, 29043, 32090, 30336, 28705,\n",
      "         37925, 30589, 37864, 29538, 32223, 31033, 30609, 28705, 28784, 29884,\n",
      "         28733, 28782, 29884, 33693,   557, 26837, 28705, 40165, 31306, 38757,\n",
      "         29288, 29614, 28705, 28781, 29884, 32072, 31673, 28725, 35048, 32015,\n",
      "         39991, 32091, 29043, 37910,   325, 28784, 28734, 29653, 29164, 28705,\n",
      "         28787, 28770, 37910,   557, 35048, 32015, 39991, 32091, 29043, 32782,\n",
      "         30387, 29324, 29404,   325, 28784, 28734, 29653, 29164, 28705, 28787,\n",
      "         28770, 37910, 28705, 28750, 28774, 29599, 37880, 28725, 28705, 28740,\n",
      "         28734, 28750, 30010, 31110, 30387, 29324, 29404,   557, 32019, 29426,\n",
      "         32091, 29043, 37910,   325, 28784, 28774, 29653, 29164, 28705, 28774,\n",
      "         28740, 37910,   557, 32073, 30446, 29324, 33264, 29828, 29700, 32509,\n",
      "         32090, 30336, 32127, 29634, 28705, 37925, 30589, 28705, 28740, 29744,\n",
      "         28725, 32073, 30446, 29324, 33264, 29828, 29700, 32509, 32090, 30336,\n",
      "         32127, 29634, 32924, 28705, 28740, 29744, 28725, 28702, 32065, 38137,\n",
      "         29599, 31091, 28705, 28781, 31479, 33693, 32004, 33142, 28725, 26837,\n",
      "         32927, 29187, 32471, 29578, 28705, 28750, 29884, 33142, 28725, 32531,\n",
      "         29470, 40055, 28705, 28781, 29653, 29164, 33693, 34046, 32046, 31073,\n",
      "         32120, 32153, 30159, 28725, 32531, 29470, 40055, 28705, 28740, 29884,\n",
      "         32072, 31673, 29844, 28705, 28740, 29884, 32403, 29687, 31673, 28725,\n",
      "         32531, 29470, 40055, 32674, 30615, 37903, 33142, 32677, 32030, 28723,\n",
      "            13, 28750, 28734, 28740, 28781, 28733, 28740, 28782, 32015, 39991,\n",
      "         32089, 32073, 30446, 29324, 33264, 29828, 29700, 32509, 28725, 32073,\n",
      "         30446, 29324, 32066, 37864, 40055, 28725, 26837, 28705, 40165, 31306,\n",
      "         38757, 29288, 29614, 28705, 28770, 32633, 32492, 32008, 33563, 32072,\n",
      "         31673, 29189, 32987, 32228, 28705, 28750, 28734, 28734, 28783, 28733,\n",
      "         28734, 28774, 32015, 39991, 29148, 32421, 32104, 35451, 32354, 30514,\n",
      "         30502, 29189, 34013, 32298, 32097, 28725, 32354, 30514, 30502, 32072,\n",
      "         31673, 29844, 32120, 30665, 29433, 26837, 28705, 40165, 31306, 38757,\n",
      "         29288, 29614, 28705, 37925, 30589, 37864, 29844, 26837, 28705, 40165,\n",
      "         31306, 38757, 29288, 29614, 32924, 37864, 28725, 32073, 30446, 29324,\n",
      "         33264, 29828, 29700, 32509, 32924, 37864, 29189, 32987, 32025, 28723,\n",
      "         34333, 37968, 32297, 32809, 28705, 28750, 28734, 28740, 28782, 31479,\n",
      "         32019, 32057, 32481, 33659, 32178, 33518, 28705, 40011, 30159, 31718,\n",
      "         33098, 37893,  9138, 32083, 31091, 38070, 29143, 36764, 32002, 39979,\n",
      "         33190, 28725,  9138, 32083, 31091, 38070, 29143, 32876, 32090, 30336,\n",
      "         28705, 28782, 32981, 26837, 28705, 40165, 31306, 38757, 29288, 29614,\n",
      "         32072, 31673, 29844, 32354, 30514, 30502, 28705, 28750, 29884, 32224,\n",
      "         32391, 29189, 34013, 32025, 28723, 28705, 28750, 28734, 28740, 28781,\n",
      "         28733, 28740, 28782, 32015, 39991, 29148, 28705, 28782, 28787, 29653,\n",
      "         29164, 28705, 28782, 28783, 37910, 29189, 32391, 29282, 32297, 32809,\n",
      "         33264, 29828, 29700, 32509, 32091, 39536, 29900, 29151, 29578, 28725,\n",
      "         26837, 32927, 29187, 32471, 29578, 32041, 33715, 32047, 29189, 28705,\n",
      "         40271, 38145, 29015, 32128, 28723, 32277, 32577, 32178, 31160, 29148,\n",
      "         32490, 29429, 29433, 28705, 28750, 28734, 28740, 28782, 31479, 28702,\n",
      "         32065, 38137, 29599, 31091, 29200, 33142, 32783, 28725, 32650, 32297,\n",
      "         33445, 28705, 28782, 32981, 28702, 32065, 38137, 29599, 31091, 32027,\n",
      "         28723,    13,  1177, 13918,    13, 27332, 35934, 38439, 32233, 28725,\n",
      "         32451, 33600, 33192, 34080, 32649, 12813,   464, 29828, 34081, 32354,\n",
      "         30514, 30502, 29189, 34013, 29282, 32015, 39991, 29538, 33548, 30263,\n",
      "          3725,    13, 27332, 21631, 28747,    13]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 35\u001b[0m     generated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m     37\u001b[0m         generated[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][prompt_size:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/generation/utils.py:1271\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1271\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/generation/utils.py:1144\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1147\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "prompt = \"\"\"\n",
    "우리는 아래와 같은 정보를 갖고 있습니다.\n",
    "---------------------\n",
    "메시는 주장으로서 2014년 FIFA 월드컵과 2022년 FIFA 월드컵에서 아르헨티나 축구 국가대표팀을 이끌며 월드컵 결승에 진출하였으며, 2014년에는 아쉽게 독일에 패배했다. 2014년 대회에서 메시는 4경기 연속 맨 오브 더 매치에 선정되는 등 뛰어난 활약을 보이며 아르헨티나 축구 국가대표팀을 준우승으로 이끌었고 7경기 4골 1도움을 기록하며 2014 FIFA 월드컵 득점 3위에 오르고 월드컵 최우수 선수상인 골든볼을 수상했다. 그러나 2022년 대회에서는 디펜딩 챔피언 프랑스를 꺾고 우승을 차지하였으며, 역사상 최초로 골든볼을 2회 수상한 선수가 되었다.\n",
    "리오넬 메시의 기록으로는 UEFA 챔피언스리그 4년 연속 득점왕, UEFA 챔피언스리그 통산 득점왕 5회(최다 통산 득점왕은 호날두 6회-5회 연속), UEFA 챔피언스리그 4회 우승, 단일 시즌 최다골 (60경기 73골), 단일 시즌 최다 공격포인트 (60경기 73골 29도움, 102공격포인트), 한해 최다골 (69경기 91골), 스페인 프리메라리가 통산 역대 득점 1위, 스페인 프리메라리가 통산 역대 도움 1위, FIFA 발롱도르 4년 연속으로 수상, UEFA 올해의 선수상 2회 수상, 월드컵 4경기 연속 맨 오브 더 매치, 월드컵 1회 우승과 1회 준우승, 월드컵 골든볼 수상 등이 있다.\n",
    "2014-15 시즌에는 스페인 프리메라리가, 스페인 국왕컵, UEFA 챔피언스리그 3개의 중요 대회에서 우승을 차지하며 2008-09 시즌에 이어 또다시 트레블을 달성하였으며, 트레블 우승과 더불어 UEFA 챔피언스리그 득점왕과 UEFA 챔피언스리그 도움왕, 스페인 프리메라리가 도움왕을 차지했다. 이로써 메시는 2015년 한 해 동안 최고의 활약을 펼치며 소속팀 FC 바르셀로나를 이끌었고, FC 바르셀로나는 통산 5번째 UEFA 챔피언스리그 우승과 트레블 2회라는 기록을 달성했다. 2014-15 시즌에 57경기 58골을 기록한 메시는 프리메라리가 최우수선수상, UEFA 올해의 선수상 등 각종 상을 싹쓸이하였다. 그리고 이러한 활약에 힘입어 2015년 FIFA 발롱도르를 수상했으며, 이는 메시의 5번째 FIFA 발롱도르이다.\n",
    "---------------------\n",
    "### 주어진 정보에 따라, 질문에 답해주세요.: '메시가 트레블을 달성한 시즌은 언제야?'\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\")\n",
    "prompt_size = len(batch[\"input_ids\"][0])\n",
    "print(\"prompt_size:\", prompt_size)\n",
    "batch = {k: v.to() for k, v in batch.items()}\n",
    "pprint(batch)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=32,\n",
    "    exponential_decay_length_penalty=(256, 1.03),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.05,\n",
    "    do_sample=True,\n",
    "    top_p=0.7,\n",
    "    min_length=5,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(**batch, generation_config=generation_config)\n",
    "    response = tokenizer.decode(\n",
    "        generated[\"sequences\"][0][prompt_size:], skip_special_tokens=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b493dd8-8050-4b1f-935f-2f69aac98ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 29871,    13,  ..., 22137, 29901,    13]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae4761b-48df-4323-b960-1cfb917aeedd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model KRAFTON/KORani-v3-13B with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKRAFTON/KORani-v3-13B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# PROMPT = '''당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# You are a helpful AI assistant, you'll need to answer users' queries in a friendly and accurate manner.'''\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# instruction = \"서울과학기술대학교 MLP연구실에 대해 소개해줘\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print(outputs[0][\"generated_text\"][len(prompt):])\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/pipelines/__init__.py:788\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;66;03m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    787\u001b[0m model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 788\u001b[0m framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    799\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/pipelines/base.py:278\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m framework, model\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model KRAFTON/KORani-v3-13B with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "model_id = \"KRAFTON/KORani-v3-13B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipeline.model.eval()\n",
    "\n",
    "PROMPT = \"\"\"당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.\n",
    "You are a helpful AI assistant, you'll need to answer users' queries in a friendly and accurate manner.\"\"\"\n",
    "instruction = \"서울과학기술대학교 MLP연구실에 대해 소개해줘\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt) :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f951eb0-4eaf-4250-8992-e935f944086e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT 변환 및 추론 성공: torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Cannot infer input type from calcuations in graph for input x.1. Assuming it is Float32. If not, specify input type explicity\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_tensorrt\n",
    "\n",
    "\n",
    "# 간단한 모델 정의 (스크립팅 가능)\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x + 1\n",
    "\n",
    "\n",
    "model = SimpleModel().cuda().eval()\n",
    "\n",
    "# JIT 스크립트 모듈로 변환\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# TensorRT 변환\n",
    "input_tensor = torch.randn((1, 3, 224, 224)).cuda()\n",
    "trt_model = torch_tensorrt.ts.compile(\n",
    "    scripted_model, inputs=[torch_tensorrt.Input(input_tensor.shape)]\n",
    ")\n",
    "\n",
    "# 변환된 모델로 추론\n",
    "with torch.no_grad():\n",
    "    output = trt_model(input_tensor)\n",
    "\n",
    "print(\"TensorRT 변환 및 추론 성공:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a215fe-f00a-4eb6-87b9-889f6286345d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyEnv 3.10.14",
   "language": "python",
   "name": "3.10.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
