{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaDelta\n",
    "\n",
    " * [ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "AdaDelta는 per-dimension learning rate method를 갖고있으며,\n",
    "첫번째     Vanilla stochastic gradient descent보다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent 의 문제.. \n",
    "\n",
    "많은 머신러닝 알고리즘들은 a set of parameters $ \\theta $ 값을 objective function $ f(\\theta) $를 optimize함으로서 update해줍니다.<br>\n",
    "이러한 알고리즘들은 보통 iterative procedure를 갖고 있으며, 매 iteration마다  change to the parameters인 $ \\Delta \\theta $를 적용시킵니다. \n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t + \\Delta \\theta_t $$\n",
    "\n",
    "이때 negative gradient $ g_t $를 사용합니다. 즉..\n",
    "\n",
    "$$ \\Delta \\theta_t = - \\eta g_t $$\n",
    "\n",
    "여기서 $ g_t $는 partial derivative이고, $ \\eta $는 learning rate를 가르킵니다.\n",
    "\n",
    "$$ g_t = \\frac{\\partial f(\\theta_t)}{\\partial \\theta_t} $$\n",
    "\n",
    "보통 SGD 또는 Mini-batch GD를 사용하게 되는데.. 문제는 learning rate를 명시해야 하며, learning rate에 따라서 결과값이 크게 영향받을수 있습니다.<br>\n",
    "즉 learning rate를 크게 잡으면 diverge 되거나 낮게 잡으면 learning이 매우 느려지게 됩니다. <br>\n",
    "따라서 Model에 대한 learning rate를 제대로 잡아야 하는데... 문제는 이게 사람이 잡는다는것이 문제. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-dimension First Order Methods \n",
    "\n",
    "### Momentum\n",
    "\n",
    "가장 중요한 핵심은 gradient $ g_t $ 지속적으로 같은 방향을 가르키며, gradient의 부호(sign...- 또는 +)가 지속적으로 변한다면.. progress를 slow시키는 것입니다. <br>\n",
    "이는 past parameter update $ \\Delta \\theta_{t-1} $, 그리고 exponential decay $ \\gamma $를 사용함으로서 가능해집니다.\n",
    "\n",
    "$$ \\Delta \\theta_t = \\gamma \\Delta \\theta_{t-1} - \\eta g_t $$\n",
    "\n",
    "여기서 $ \\gamma $는 constant값으로서 이전 parameter update $ \\Delta \\theta_{t-1} $을 control 합니다.<br>\n",
    "\n",
    "SGD에서 progress는 매우 느립니다. 왜냐하면 gradient magnitude은 매우 작으며, <br>\n",
    "fixed global learning rate (게다가 모든 dimensions에 적용) progress를 더더욱 느리게 만들어버립니다.\n",
    "\n",
    "높은 learning rate를 사용하면 valley를 서로 왔다갔다하는 oscillations을 만들수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "update rule은 다음과 같습니다. \n",
    "\n",
    "$$ \\Delta \\theta_t = - \\frac{\\eta}{\\epsilon + \\sqrt{ \\sum^t_{τ=1} g^2_τ }} \\odot g_t $$\n",
    "\n",
    "여기서 denominator의 경우 l2 norm 의 모든 이전의 gradients를 연산합니다. <br>\n",
    "$ \\eta $의 경우 global learning rate입니다.\n",
    "\n",
    "물론 global learning rate가 있지만, 각각의 $ \\theta $의 dimension마다 각각의 dynamic rate를 갖고 있습니다. <br>\n",
    "dynamic rate는 gradient magnitude와 반대로 커나가기 때문에, large gradient의 경우 작은 learning rate갖으며, small gradient의 경우에는 large learning rate를 갖습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Order Methods\n",
    "\n",
    "## AdaDelta\n",
    "\n",
    "먼저 AdaGrad의 문제점은 다음과 같습니다. \n",
    "\n",
    "1. Training동안 지속적으로 작아지는 learning rate (continual decay of learning rates throughout training)\n",
    "2. Global Learning Rate의 선택 필요 \n",
    "\n",
    "### exponentially decaying average of the squared gradients\n",
    "sum of squared gradients를 accumulate하는 것이 아니라, AdaDelta는 window of accumulated past gradients를 제한함으로서 고정된 싸이즈의 w값을 유지합니다. 이전의 squared gradients값 w를 저장하는것은 비효율적이기 때문에, accumulation은 exponentially decaying average of the squared gradients로 구현을 합니다.\n",
    "\n",
    "[원문] Since storing $ w $ previous squared gradients is inefficient, our methods implements this accumulation as an exponentially decaying average of the squared gradients. Assume at\n",
    "time t this running average is $ E[g^2]_t $ then we compute\n",
    "\n",
    "$$ E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g^2_t $$\n",
    "\n",
    "여기서 $ \\gamma $는 Momentum method에서 사용된 것과 유사한 decay constant입니다.<br>\n",
    "update시에 square root을 해줘야 하며, 궁극적으로 이전 squared gradients (t시간 까지의)의 RMS가 됩니다. \n",
    "\n",
    "$$ RMS[g]_t = \\sqrt{E[g^2]_t + \\epsilon} $$\n",
    "\n",
    "* [RMS참고 - Wolfram MathWorld](http://mathworld.wolfram.com/Root-Mean-Square.html)\n",
    "\n",
    "궁극적으로 Denominator는 $ RMS[g]_t $로 대체 될수 있습니다.\n",
    "\n",
    "$$ \\Delta \\theta_t =  \\frac{\\eta}{RMS[g]_t} \\odot g_t $$\n",
    "\n",
    "### Exponentially decaying average of the squared weights\n",
    "위의 공식 분모에서 mean squared gradient를 구했습니다.<br>\n",
    "이때 분자에서도 동일한 units으로 계산이 되어야 합니다. 따라서 mean squared weight 를 구합니다.\n",
    "\n",
    "$$ E[\\Delta \\theta^2]_t = \\gamma E[\\Delta \\theta^2]_{t-1} + (1-\\gamma) \\Delta \\theta^2_t  $$\n",
    "\n",
    "위의 root mean squared error of weight updates는 다음과 같습니다.\n",
    "\n",
    "$$ RMS[\\Delta \\theta]_t = \\sqrt{E[\\Delta \\theta^2]_t + \\epsilon} $$\n",
    "\n",
    "일단 $ E[\\Delta \\theta^2]_t $는 모르는 상태이기 때문에, 이전 update rule인 $ E[\\Delta \\theta]_{t-1} $로 approximate해주면 AdaDelta 공식이 나오게 됩니다.<br>\n",
    "\n",
    "$$ \\begin{align} \n",
    "\\Delta \\theta_t &= - \\frac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_t} \\odot g_t  \\\\\n",
    "\\theta_{t+1} &= \\theta_t + \\Delta \\theta_t\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "* **Require**: Decay Rate $ \\gamma $, Epsilon Constant $ \\epsilon $\n",
    "* **Initialization**: $ \\theta 초기화 $, accumulation variables $ E[g^2]_0 = 0 $, $ E[\\Delta \\theta^2]_0 = 0 $\n",
    "\n",
    "\n",
    "1. for t = 1: T do: <br>\n",
    "  1. Compute Gradient: $ g_t$\n",
    "  2. Accumulate Gradient: $$ E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g^2_t $$\n",
    "  3. Compute Update: $$ \\Delta \\theta_t = - \\frac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_t} \\odot g_t $$\n",
    "  4. Accumulate Updates:  $$ E[\\Delta \\theta^2]_t = \\gamma E[\\Delta \\theta^2]_{t-1} + (1-\\gamma)\\Delta \\theta^2_t $$\n",
    "  5. Apply Update: $$ \\theta_{t + 1} = \\theta_t + \\Delta \\theta_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc80abefc50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XucFPWZ7/HPw4hC5KbiISAqXjEKyMiIGnWFeMF1ibAR\nReKNqMdjXNyzm9UTXD1KWLO6sqvR6MbVxOBmfYl3YlCDBp14jzKAgBfQGBVG1gsEnVFQmHn2j64Z\ne4bunqqZrqnq7u/79ZoXU7+qqX76N9AP9Xvq9ytzd0RERMLokXQAIiJSOpQ0REQkNCUNEREJTUlD\nRERCU9IQEZHQlDRERCQ0JQ2RkMzsDDN7POk4RJKkpCESMLN3zOy4fPvd/S53P6ET5601s81m1mBm\nn5pZnZnNNLMdIpzDzWzfqK8tUmxKGiIhmNl2XTzFDHfvCwwG/gE4HXjUzKzLwYl0IyUNkRzMbLqZ\nPWdmN5jZemBW0PZssN+CfR8GVw8rzGxER+d198/cvRY4GTgC+KvgfGPN7AUz22hm68zsZjPbPtj3\ndPDjr5hZo5lNNbOdzGyBmX1kZn8Ovh8aR1+IZFPSEMnvMOBtYBDw43b7TgD+Atgf6A+cBqwPe2J3\nfw9YDBwdNDUBfw8MJJNMjgUuCo79i+CYg929j7vfQ+bf7i+BPYE9gE3AzdHenkh0Shoi+b3v7j91\n963uvqndvi1AX+AAwNz9dXdfF/X8wM4A7l7n7i8Gr/UO8B/AMfl+0N3Xu/sD7v65uzeQSWp5jxcp\nFiUNkfzW5Nvh7k+S+Z/9LcCHZnabmfWLeP7dgA0AZrZ/MMT032b2KfDPZK46cjKzr5nZf5jZu8Hx\nTwMDzKwqYgwikShpiORXcAlod7/J3ccAB5IZpro07InNbHdgDPBM0PQz4A1gP3fvB/wjUKhI/g/A\ncOCw4PiWISwV1iVWShoinWBmh5rZYWbWE/gM2Aw0h/i5r5nZMcCvgZeAR4NdfYFPgUYzOwD4frsf\n/QDYO2u7L5k6xkYz2xm4qivvRyQsJQ2RzukH3A78GXiXTBF8ToHjbzazBjIf/j8BHgBOdPeWRHMJ\n8F2gITjvPe1+fhZwZ3B31WnBOXoDHwMvAr8twnsS6ZDpIUwiIhKWrjRERCQ0JQ0REQlNSUNEREJT\n0hARkdC6ughb6gwcONCHDRvWuv3ZZ5+x4447JhdQyql/8lPfFKb+ya8U+6auru5jd9+1o+PKLmkM\nGzaMxYsXt27X1tYybty45AJKOfVPfuqbwtQ/+ZVi35jZu2GO0/CUiIiEpqQhIiKhKWmIiEhoZVfT\nyGXLli2sXbuWzZs3Jx1K6vTv35/XX3+9qOfs1asXQ4cOpWfPnkU9r4gkryKSxtq1a+nbty/Dhg1D\nT9dsq6Ghgb59+xbtfO7O+vXrWbt2LXvttVfRzisi6VARw1ObN29ml112UcLoBmbGLrvsoqs6kTJV\nEUkDUMLoRurrErL8XrhhBMwakPlz+b1JRyQpVxHDUyKSw/J74Td/C1uCJ9l+siazDTDqtOTiklSr\nmCuNNPjxj3/MQQcdxKhRoxg9ejR/+MMf8h47d+5c3n///W6MTirOotlfJYwWWzZl2kXySPRKw8zu\nACYCH7r7iBz7x5F5wtmfgqYH3b0k/0a/8MILLFiwgCVLlrDDDjvw8ccf8+WXX+Y9fu7cuYwYMYIh\nQ4Z0Y5RSUT5ZG61dhOSvNOYCJ3ZwzDPuPjr46paEMX9pPUde+yR7zXyEI699kvlL67t8znXr1jFw\n4EB22GEHAAYOHMiQIUOoq6vjmGOOYcyYMUyYMIF169Zx//33s3jxYs444wxGjx7Npk2bWLRoEdXV\n1YwcOZJzzz2XL774AoCZM2dy4IEHMmrUKC655BIAfvOb33DYYYdRXV3NcccdxwcffNDl+KUM9R8a\nrV2EhJOGuz8NbEgyhvbmL63nsgdXUL9xEw7Ub9zEZQ+u6HLiOOGEE1izZg37778/F110Eb///e/Z\nsmULF198Mffffz91dXWce+65XH755UyZMoWamhruuusuli1bhpkxffp07rnnHlasWMHWrVv52c9+\nxvr163nooYd49dVXWb58OVdccQUARx11FC+++CJLly7l9NNP57rrritCz0jZOfZK6Nm7bVvP3pl2\n6ViF3kRQCoXwI8zsFeB94BJ3fzXOF5uzcBWbtjS1adu0pYk5C1cxuXq3Tp+3T58+1NXV8cwzz/DU\nU08xdepUrrjiClauXMnxxx8PQFNTE4MHD97mZ1etWsVee+3F/vvvD8A555zDLbfcwowZM+jVqxfn\nnXceEydOZOLEiUBmXsrUqVNZt24dX375peZLSG4txe5FszNDUv2HZhKGiuAdq+CbCNKeNJYAe7p7\no5mdBMwH9mt/kJldAFwAMGjQIGpra1v3NTY20r9/fxoaGkK94PsbN+VtD3uOQsaMGcOYMWPYd999\nuf322znggANYtGhRm2MaGhpoamris88+o6Ghgc8++4ympqbW1//888/ZunVr67BVbW0tDz30EDfe\neCMLFizgoosuYsaMGZx00kk888wzXHPNNXljzz5vMW3evLnN76EUNTY2lvx76Nj/guqbv9rcAIR8\nz5XRP3l8+DHsPXPb9jc+hg21Zd03qU4a7v5p1vePmtm/m9lAd/+43XG3AbcB1NTUePaSxLW1tfTq\n1Sv0rOchA3pTnyNxDBnQu0szp1etWkWPHj3Yb7/9WrdHjBjB448/zsqVKzniiCPYsmULq1ev5qCD\nDmLAgAE0NzfTt29fDjnkENasWcMHH3zAvvvuywMPPMCxxx6LmdHc3MyUKVM4/vjj2Xvvvenbty+N\njY3su+++9O3bl/vuu4+qqqq8sRd7RniLXr16UV1dXfTzdqdSXN66O1V0/8yaDHiOHQanbSzrvkm6\nEF6QmX3dgpliZjaWTLzr43zNSycMp3fPqjZtvXtWcemE4V06b2NjI+ecc05r0fq1115j9uzZ3H//\n/fzwhz/k4IMPZvTo0Tz//PMATJ8+nQsvvJDRo0fj7vzyl7/k1FNPZeTIkfTo0YMLL7yQhoYGJk6c\nyKhRozjqqKO4/vrrAZg1axannnoqY8aMYeDAgV2KW6RblFp9IM6bCFLeF0nfcns3MA4YaGZrgauA\nngDufiswBfi+mW0FNgGnu3uu9F40LXWLOQtX8f7GTQwZ0JtLJwzvUj0DMsNSLQkh28CBA3n66ae3\naT/llFM45ZRTWrePPfZYli5d2uaYwYMH89JLL23zs5MmTWLSpEldilek25RifeDYK9vGDMW5iaAE\n+iLRpOHu0zrYfzNwc6Fj4jC5ercuJwkRCanQJMOUfFBuI66bCEqgL1Jd0xCRClCqkwxHnVb8D/IS\n6ItU1zREpAJokuFXSqAvlDREyk3KC6nbqIRJhmF/JyXQFxqeEiknJVBI3Ua5TzKM8jspgb5Q0hAp\nJyVQSM0pjvpAWkT9naS8LzQ81U3Gjx/PwoUL27T95Cc/4fvf/36XznvllVfyu9/9LvLP1dbWti47\nImWkBAqpFafMfidKGt1k2rRpzJs3r03bvHnzmDat4F3HQOa5283NzTn3zZ49m+OOO64oMXZGodgk\nASVQSK04ZfY7UdLIJYZC4pQpU3jkkUdan6Hxzjvv8P7773P00UczZ84cDj30UEaNGsVVV13Vun/4\n8OGcffbZjBgxgjVr1jB9+nRGjBjByJEjueGGG4DMzPH7778fgJdffplvfvObHHzwwYwdO5aGhgY2\nb97M9773PUaOHEl1dTVPPfXUNrFt2LCByZMnM2rUKA4//HCWL18OZGaW/+u//mvrcSNGjOCdd97J\nGZukRNyF1AU/gB/tDLP6w7plme1iKLXiPaSjuJ1Av6mm0V5MhcSdd96ZsWPH8thjjzFp0iTmzZvH\naaedxhNPPMGbb77JSy+9hLtz8skn8/TTT7PHHnvw5ptvcuedd3L44YdTV1dHfX09K1euBGDjxo1t\nzv/ll18ydepU7rnnHg499FA+/fRTevfuzY033oiZsWLFCt544w1OOOEEVq9e3eZnr7rqKqqrq5k/\nfz5PPvkkZ599NsuWLSv4frJjkxSJs5C64Aew+Bdt21q2J17f+fOWYvE+DcXthPpNVxrtxfgIzOwh\nqpahqccff5zHH3+c6upqDjnkEN544w3efPNNAPbcc8/WD+W9996bt99+m4svvpjf/va39OvXr825\nV61axeDBgzn00EMB6NevH9tttx3PPvssZ555JgAHHHAAe+655zZJ49lnn+Wss84C4Fvf+hbr16/n\n008/pZDs2CRlRp0Gf78SZm3M/FmsD5C6udHawyrFx85GjTmO30lC/aak0V6MRatJkyaxaNEilixZ\nwueff86YMWNwdy677DKWLVvGsmXLeOuttzjvvPMA2HHHHVt/dqedduKVV15h3Lhx3HrrrZx//vld\njqcj2223XZt6xebNm1u/z45NKoQ3RWsPqxQLxWmIOaEYlDTai7Fo1adPH8aPH8+5557bWgCfMGEC\nd9xxB42NjQDU19fz4YcfbvOzH3/8Mc3NzZxyyilcffXVLFmypM3+4cOHs27dOl5++WUgs+T51q1b\nOfroo7nrrrsAWL16Ne+99x7Dh7ddsTf7mNraWgYOHEi/fv0YNmxY6+ssWbKEP/3pT0gFs6po7WGV\nYqE4DTEnFIOSRnsxFxKnTZvGK6+80po0TjjhBL773e9yxBFHMHLkSKZMmZLzoUj19fWMGzeO0aNH\nc+aZZ3LNNde02b/99ttzzz33cPHFF3PwwQdz/PHHs3nzZi666CKam5sZOXIkU6dOZe7cua3PKW8x\na9Ys6urqGDVqFDNnzuTOO+8EMivtbtiwgYMOOoibb7659cmBUqHGTI/WHlaaCsVpKG6HlVAMFvNK\n492upqbGFy9e3LpdW1vLoEGD+MY3vhH+JMvvTfWMzGKK6yFMr7/+erQ+T6FyfpBOpyy/F+ZfCM2Z\n4aja4T9i3JuzYfKtxSnqxl0ohsyH6rdvyn3uzhyfJ+Zu+7tTxH4zszp3r+noON09lUvKZ2SKJGLR\n7NaE0aq5qTizzeP4Nxd1JnYpztxOIAYNT4lIOGko/kYRNd5Se38JqZikUW7DcGmmvi5TaSj+RhE1\n3lJ7fwmpiKTRq1cv1q9frw+zbuDurF+/nl69eiUdSn5xzqKNcu40zILOnuH9o50Lz/COWniN6/3F\nVaxOQ3G7BFRETWPo0KGsXbuWjz76KOlQUmfz5s1F/4Dv1asXQ4em9H9ncc6ijXLuNMyCbj/D25sK\nz/BuP7O5avvwReVivb84Z2KXwLLkaVARd0/pDpj8Kq5/bhiR+aBpr//umZm6WSL3TYRzRzo2Lj/a\nOffEPKuCqzZ0+OMF+yeu95eGfguhFP9dhb17qiKGp0RaxVnsjHLuNBRd45rhDfG9vzT0W4VT0pDK\n0pliZ9gx9CjnTkPRNa4Z3hD9/cXRx51RajWpBChpSGXpTDH3N38bDIn4V2PouT4gopw7DUXXuGZ4\nQ7T3F1cfRxUljijHlhklDakso07LFG/77w5Y5s98xVyItpJolHNHjSMOE6+HmvO+urKwqsx2V5Y5\nbxHl/cXVx1FFiaMUV+Ytkoq4e0qkjSizaKOOoUc5dxpmFE+8vjhJIpew7y/OPo6i1GpSCdGVhkgh\naag9lLu09HGp1aQSkmjSMLM7zOxDM8t5r5xl3GRmb5nZcjM7pLtjlAqXhtpDqYprEl5cBehSq0kl\nJOkrjbnAiQX2/yWwX/B1AfCzbohJ5CtpqD2UoiiF4ih9HGcButRqUglJtKbh7k+b2bACh0wC/tMz\nMxBfNLMBZjbY3dd1S4AikI7aQ6mJa8XYqOeNqtRqUglIfEZ4kDQWuPuIHPsWANe6+7PB9iLgh+6+\nuN1xF5C5EmHQoEFjWp7DDdDY2EifPn1ii7/UqX/yU98UVrB/1i3L/4ODR3f+ReM6b5GV4t+d8ePH\nV87zNNz9NuA2yCwjkj19vxSn83cn9U9+6pvCCi8jMiP/ch/TurKMSEznLbJy/ruTdE2jI/XA7lnb\nQ4M2kXSKUqSNssJsXDHEJa5CcQUXoNMi7UnjYeDs4C6qw4FPVM+Q1IpSpG1ZYbZlnaeWFWa7mjjS\nMlM5rkJxBReg0yLR4SkzuxsYBww0s7XAVUBPAHe/FXgUOAl4C/gc+F4ykYqEEKVIWzc39znq5nZt\nsl3cheIo4ioUV2gBOi2SvntqWgf7HfibbgpHpGuizBKOa4XZCp6pLN0j7cNTIqUjyizhuFaYreCZ\nytI9lDREiuXYKzNPs8tWtX3uIm3UFWbjml0tEpGShkgxtZ/3lG8e1B6HQ492VxU9qjLt7cU1u1qk\nE8pinoZIKiyaDc1b2rY1b8ldhF40G5rb1S+am/IfG8fsapFO0JWGSLHEtbS2ituSIkoaIsUS19La\nKm5LiihpiHQk7MztuJbWPvZK6NGzbVuPnsV7xGmU2ePZx3/4WkU83lTaUk1DpJCWmdstWmZuw7aT\n8FrqCItmZ4aO+g/NfLDnqzuEPRbArPB2Z7QU2FvqJS0F9uz4Ch3f9GXh46UsKWmIFBJ15nYcS2sv\nmp35gM7W9GXXZ3lHLbCnaba5JEbDUyKFxDVzO4q4CuFRz6uCvKCkIWmWhtVao87cjiPmuArhUc+r\ngrygpCFplZbVWqPM3I4r5v1OiNYeVtTZ45ptLihpSFoVGj/vThOvh5rzvrqysKrMdq56Rlwxv/l4\ntPawos4eb3981faabV6BVAiXdErT+PnE68MtV56W2kMUUWePZx9fWwujxnU9BikputKQdCrF8fO0\n1B5EYqSkIemUpvHzpFeYTVNfSMXT8JSkU9TJb3GJMgEurpjT0hciKGlImqVhtda0rDCbhr4QQcNT\nIoWlqSAvkgJKGiKFqAgt0oaShkghca4wK1KClDREOhLHCrMiJUpJQ6SQQivMilQgJQ2RQlQIF2lD\nSUOkEBXCRdpINGmY2YlmtsrM3jKzmTn2Tzezj8xsWfB1fhJxSgWLOhs7Dcu5i8Sow8l9ZrYDcAow\nLPt4d+/SoK6ZVQG3AMcDa4GXzexhd3+t3aH3uPuMrryWSKdFmY0d9fGpIiUozIzwXwOfAHXAF0V8\n7bHAW+7+NoCZzQMmAe2ThkiyojyWVY9DlTJn7l74ALOV7j6i6C9sNgU40d3PD7bPAg7Lvqows+nA\nNcBHwGrg7919TY5zXQBcADBo0KAx8+bNa93X2NhInz59ih1+2VD/5Be5b9Yty79v8OiuB5Qy+ruT\nXyn2zfjx4+vcvaaj48JcaTxvZiPdfUUR4orqN8Dd7v6Fmf0f4E7gW+0PcvfbgNsAampqfNy4ca37\namtryd6WtorSP8vvLcvF9CL3zQ0zgqf2tdN/d5i2ctv2Eu83/dvKr5z7Jm8h3MxWmNly4ChgSVCw\nXp7V3lX1wO5Z20ODtlbuvt7dW4bEfg6MKcLrSjGl5bGsaRClaK5+kxJV6EpjYsyv/TKwn5ntRSZZ\nnA58N/sAMxvs7uuCzZOB12OOSaLSOP5XohTN1W9SovImDXd/F8DMfuXuZ2XvM7NfAWfl/MGQ3H2r\nmc0AFgJVwB3u/qqZzQYWu/vDwN+a2cnAVmADML0rrykx0OS3tsIWzdVvUqLC1DQOyt4IbpUtyjCR\nuz8KPNqu7cqs7y8DLivGa0lM+g/NM46vyW8Fqd+kRBWqaVxmZg3AKDP7NPhqAD4kcxuuiB5F2lnq\nNylReZOGu1/j7n2BOe7eL/jq6+67BFcAIpmhmG/flLlDCMv8+e2bNC7fEfWblKgww1P3mdkh7do+\nAd51960xxCSlRo8i7Rz1m5SgMEnj34FDgOWAASOBlUB/M/u+uz8eY3wiIpIiYRYsfB+odvcadx8D\njAbeJrNm1HVxBiciIukSJmns7+6vtmwECwoe0LJmlJShqCu1amVXkYoRZnjqVTP7GdCyoNNU4LVg\n9dstsUUmyYi6UqtWdhWpKGGuNKYDbwF/F3y9HbRtAcbHFZgkpNBM5WIcLyIlrcMrDXffBPxb8NVe\nY9EjkmRFnamsmc0iFaXDKw0zO9LMnjCz1Wb2dstXdwQnCYj6eFM9DlWkooQZnvoFcD2Z1W4PzfqS\nchR1prJmNotUlDCF8E/c/bHYI5F0iLJSa2eOF5GSFiZpPGVmc4AHyXrcq7sviS0qSVbUmcqa2SxS\nMcIkjcOCP7MfA+jkeIKeiIiUtzB3T+m2WhERAcLdPTXIzH5hZo8F2wea2XnxhyYiImkT5u6puWSe\nrjck2F5NZpKfiIhUmDBJY6C73ws0Q+YxrUBTrFGJiEgqhUkan5nZLmSK35jZ4WSepyEiIhUmzN1T\nPwAeBvYxs+eAXYEpsUYl5Wv5vZrTIVLCwtw9tcTMjgGGk3kI0yp31+q2Ep1WxBUpeXmThpl9J8+u\n/c0Md38wppikXBVaEVdJQ6QkFLrS+HaBfU5mhrhIeFoRV6Tk5U0a7v697gxEKkD/oZkhqVztIlIS\nwtw9JVIcWhFXpOQpaUj3GXUafPsm6L87YJk/v32T6hkiJSTMLbexMbMTgRuBKuDn7n5tu/07AP8J\njAHWA1Pd/Z3ujlOKSCviipS0ztw9BdDlu6fMrAq4BTgeWAu8bGYPu/trWYedB/zZ3fc1s9OBfwGm\nduV1RUSk85K8e2os8Ja7vw1gZvOASUB20pgEzAq+vx+42czM3b2Lry0iIp2Q5N1TuwHZt9Ks5atn\nd2xzjLtvNbNPgF2Aj7MPMrMLgAsABg0aRG1tbeu+xsbGNtvSlvonP/VNYeqf/Mq5b0LVNMzsr4CD\ngF4tbe4+O66gonL324DbAGpqanzcuHGt+2pra8nelrbUP/mpbwpT/+RXzn0T5nkat5KpI1xMZhmR\nU4E9i/Da9cDuWdtDg7acx5jZdkB/MgVxERFJQJhbbr/p7meTKUj/CDgC2L8Ir/0ysJ+Z7WVm2wOn\nk1kYMdvDwDnB91OAJ1XPEBFJTpjhqZbFgj43syFk/qc/uKsvHNQoZpB5wFMVcIe7v2pms4HF7v4w\n8AvgV2b2FrCBTGIREZGEhEkaC8xsADAHWELmzqmfF+PF3f1R4NF2bVdmfb+ZzHCYiIikQJikcZ27\nfwE8YGYLyBTDN8cbloiIpFGYmsYLLd+4+xfu/kl2m4iIVI5CM8K/TmaeRG8zqyZz5xRAP+Br3RCb\niIikTKHhqQnAdDK3wl6f1f4p8I8xxiQiIilVaEb4ncCdZnaKuz/QjTGJiEhKhalpPGdmvzCzxwDM\n7EAzOy/muEREJIXCJI1fkplLMSTYXg38XWwRiYhIaoVJGgPd/V6gGTKT8oCmWKMSEZFUCpM0PjOz\nXchM6sPMDgc+iTUqERFJpTCT+35AZg2ofczsOWBXMutAiYhIhekwabj7EjM7BhhOZq7GKnffEntk\nIiKSOh0mDTPrBVwEHEVmiOoZM7s1WBdKREQqSJjhqf8EGoCfBtvfBX6FFhIUEak4YZLGCHc/MGv7\nKTN7Le/RIiJStsLcPbUkuGMKADM7DFgcX0giIpJWYa40xgDPm9l7wfYewCozWwG4u4+KLToREUmV\nMEnjxNijEBGRkhDmltt3uyMQERFJvzA1DREREUBJQ0REIlDSEBGR0JQ0REQkNCUNEREJTUlDRERC\nU9IQEZHQEkkaZrazmT1hZm8Gf+6U57gmM1sWfD3c3XGKiEhbSV1pzAQWuft+wKJgO5dN7j46+Dq5\n+8ITEZFckkoak4A7g+/vBCYnFIeIiERg7t79L2q20d0HBN8b8OeW7XbHbQWWAVuBa919fp7zXQBc\nADBo0KAx8+bNa93X2NhInz59iv8myoT6Jz/1TWHqn/xKsW/Gjx9f5+41HR0XZsHCTjGz3wFfz7Hr\n8uwNd3czy5e59nT3ejPbG3jSzFa4+x/bH+TutwG3AdTU1Pi4ceNa99XW1pK9LW2pf/JT3xSm/smv\nnPsmtqTh7sfl22dmH5jZYHdfZ2aDgQ/znKM++PNtM6sFqoFtkoaIiHSPpGoaDwPnBN+fA/y6/QFm\ntpOZ7RB8PxA4EtATA0VEEpRU0rgWON7M3gSOC7Yxsxoz+3lwzDeAxWb2CvAUmZqGkoaISIJiG54q\nxN3XA8fmaF8MnB98/zwwsptDExGRAjQjXEREQlPSEBGR0JQ0REQktERqGtL95i+tZ87CVby/cRND\nBvTm0gnDmVy9W8XGISKdo6RRAeYvreeyB1ewaUsTAPUbN3HZgysA2GYafkJxKHGIlAYNT1WAOQtX\ntX5Qt9i0pYk5C1dVZBwi0nlKGhXg/Y2bIrWXexwi0nlKGhVgyIDekdrLPQ4R6TwljQpw6YTh9O5Z\n1aatd88qLp0wvCLjEJHOU9KoAJOrd+Oa74xktwG9MWC3Ab255jsju734nJY4RKTzdPdUhZhcvVsq\nPpzTEoeIdI6uNEREJDRdaUi3SsPkvivmr+DuP6yhyZ0qM6YdtjtXT+7+tTHT0BciUSlpSLdJw+S+\nK+av4L9efK91u8m9dbs7E0ca+kKkMzQ8Jd0mDZP77v7DmkjtcUlDX4h0hpKGdJs0TO5r8tyPo8/X\nHpc09IVIZyhpSLdJw+S+KrNI7XFJQ1+IdIZqGiUsSiE1zuLvGbe/wHN/3NC6feQ+O3PX/z5im+Mu\nnTCcS+97hS3NX/2vvmcP69bJfdMO271NTSO7vTtdOmF4m5oGaKKjlAYljRIVpZBaqPh7XBeXuW2f\nMACe++MGzrj9hZyJg/b/oe/e/+C3Jsqk755q+R3p7ikpNUoaJapQIbX9B0+h4u9xE77WpTjaJ4xC\n7XMWrmJLU9vawZYmzxlznK6ePDKRW2zb00RHKUWqaZSoKIVUFX9FpFiUNEpUlEKqir8iUiwankqZ\nsMXtKIXUwsXf9du0hy1st+zLNRR15D4754z5B/cuI6sOTg8jb/E3SvE+LTcFRKEZ4VKKdKWRIi3F\n7fqNm3C+Km7PX1q/zbFRVoy9evJIzjx8j9Yriyozzjx8j5wflIUK27mcWrNH6Pb7Fr/XJmEANHum\nvb2W4n3LEFpL8f6K+Su2OTZKv0U5b5yixCySJrrSSJEoxW2IVkgNW/yNUtgG8s5gzhVzlHMXKt63\nfx/FuimIrt19AAAJjklEQVSgO682ov6uRdJCVxopUoqF4rhijlK8100BIt0nkaRhZqea2atm1mxm\nNQWOO9HMVpnZW2Y2sztjTEIpForjijlK8V43BYh0n6SuNFYC3wGezneAmVUBtwB/CRwITDOzA7sn\nvMLmL63nyGufZK+Zj3DktU8WbRz60gnDt/mF9CB/oTiKK+avYJ/LHmXYzEfY57JH847h5ypgF2q/\ndMJwerT7vM1X3I5y7nwztHO1R3mMbJTztojj961H30qpSiRpuPvr7t7Rcp5jgbfc/W13/xKYB0yK\nP7rC4ixgLn53A83t2pqD9q6IUvyNUtgmiC1XcTtXzHvt2ifnOXK11+y5c85kVLPntgkmrpsCIL7f\ntx59K6XKvJvHctu8uFktcIm7L86xbwpworufH2yfBRzm7jNyHHsBcAHAoEGDxsybN691X2NjI336\n5P6w6oxV/93Al03tP9ph+6oeDP963y6de2X9pzjb/j4MY8Ru/WI577D+Pdr0T9T3FyXmKMfG2c9h\nNTY2Ut/oiceRVsX+t1VOSrFvxo8fX+fuecsFLWK7e8rMfgd8Pceuy93918V8LXe/DbgNoKamxseN\nG9e6r7a2luztrvrezEfwHBdoBvzp2q69zvSZj+Td984ZnT93ofPOPXHHNv0T9f1FiTnKsXH2c1i1\ntbVc++xniceRVsX+t1VOyrlvYksa7n5cF09RD2QPNA8N2hI1ZEBv6nPc4VKMAmaVWc67eLpapI1y\n3qjvL8q544wjLmmJQyQt0nzL7cvAfma2l5ltD5wOPJxwTJ0qVoctpEYt0sZx3qgF2ijnjjOOKKIU\nti+dMJye7Yor3b2cu0iaJHXL7V+b2VrgCOARM1sYtA8xs0cB3H0rMANYCLwO3OvuryYRb7aoxeoo\nhdQoRdq4zhu1QBulYB1nHGF1qrCd8HLuImmSaCE8DjU1Nb548Vd19WKPLe5z2aN5h1j+eM1J27Qf\nee2TOYc3dhvQm+dmfqvTcRTrvF3tn7jeX1yixFtbW8vlLzaX1PvrTuU8bt9Vpdg3ZhaqEJ7m4alU\nijqjOK6Zv2mZUZyWOMKKGm+pvT+RuClpRBR1RnFcM3/TMqM4LXGEFTXetLy/uCaUikSlpBFR1GJ1\nXAXdtMwoHn/ArpHakxa139LQz1oRV9JEq9xGFPUZ03E9Czotz5h+6o2PIrUnLWq/paGftSKupImS\nRidEfcZ0XM+CTsMzpktxzD9qvyXdz6XYx1K+NDwlXZKWMf9ypj6WNFHSCKjQ2DlpGPMvd+pjSRMN\nT/FVobFl3Lil0AgkPvyTdmkY8y936mNJEyUNVGjsqqTH/CuB+ljSQsNTqNAoIhKWkgYqNIqIhKWk\nQboKjSrIi0iaqaZBegqNKsiLSNopaQTSUGhUQV5E0k7DUymigryIpJ2SRoqoIC8iaaekkSJpKsiL\niOSimkaKpKUgLyKSj5JGyqShIC8iko+Gp0REJDQlDRERCU1JQ0REQlPSEBGR0JQ0REQkNCUNEREJ\nzdw96RiKysw+At7NahoIfJxQOKVA/ZOf+qYw9U9+pdg3e7r7rh0dVHZJoz0zW+zuNUnHkVbqn/zU\nN4Wpf/Ir577R8JSIiISmpCEiIqFVQtK4LekAUk79k5/6pjD1T35l2zdlX9MQEZHiqYQrDRERKRIl\nDRERCa0ikoaZzTGzN8xsuZk9ZGYDko4pTczsVDN71cyazawsbxOMysxONLNVZvaWmc1MOp40MbM7\nzOxDM1uZdCxpY2a7m9lTZvZa8G/q/yYdU7FVRNIAngBGuPsoYDVwWcLxpM1K4DvA00kHkgZmVgXc\nAvwlcCAwzcwOTDaqVJkLnJh0ECm1FfgHdz8QOBz4m3L7u1MRScPdH3f3rcHmi8DQJONJG3d/3d1X\nJR1HiowF3nL3t939S2AeMCnhmFLD3Z8GNiQdRxq5+zp3XxJ83wC8DpTVU9UqImm0cy7wWNJBSKrt\nBqzJ2l5Lmf3Dl/iZ2TCgGvhDspEUV9k87tXMfgd8Pceuy93918Exl5O5fLyrO2NLgzD9IyLFYWZ9\ngAeAv3P3T5OOp5jKJmm4+3GF9pvZdGAicKxX4OSUjvpH2qgHds/aHhq0iXTIzHqSSRh3ufuDScdT\nbBUxPGVmJwL/DzjZ3T9POh5JvZeB/cxsLzPbHjgdeDjhmKQEmJkBvwBed/frk44nDhWRNICbgb7A\nE2a2zMxuTTqgNDGzvzaztcARwCNmtjDpmJIU3DQxA1hIppB5r7u/mmxU6WFmdwMvAMPNbK2ZnZd0\nTClyJHAW8K3gs2aZmZ2UdFDFpGVEREQktEq50hARkSJQ0hARkdCUNEREJDQlDRERCU1JQ0REQlPS\nECkyMxtnZgvCthfh9SZnL4pnZrVarVjioqQhUvomk1mNVyR2ShpSccxsRzN7xMxeMbOVZjY1aB9j\nZr83szozW2hmg4P2WjO7MZiotdLMxgbtY83sBTNbambPm9nwiDHcYWYvBT8/KWifbmYPmtlvzexN\nM7su62fOM7PVwc/cbmY3m9k3gZOBOUF8+wSHnxoct9rMji5S14mUz9pTIhGcCLzv7n8FYGb9g/WC\nfgpMcvePgkTyYzKrIgN8zd1Hm9lfAHcAI4A3gKPdfauZHQf8M3BKyBguB55093ODh4K9FCwqCTCa\nzOqoXwCrzOynQBPw/4FDgAbgSeAVd3/ezB4GFrj7/cH7AdjO3ccGs5GvArT2mBSFkoZUohXAv5nZ\nv5D5sH3GzEaQSQRPBB+6VcC6rJ+5GzLPkjCzfsEHfV/gTjPbD3CgZ4QYTgBONrNLgu1ewB7B94vc\n/RMAM3sN2BMYCPze3TcE7fcB+xc4f8tCeXXAsAhxiRSkpCEVx91Xm9khwEnA1Wa2CHgIeNXdj8j3\nYzm2/wl4yt3/Onh2Qm2EMAw4pf3Dr8zsMDJXGC2a6Ny/05ZzdPbnRXJSTUMqjpkNAT539/8C5pAZ\n8lkF7GpmRwTH9DSzg7J+rKXucRTwSXAl0J+vlkyfHjGMhcDFwaqomFl1B8e/DBxjZjuZ2Xa0HQZr\nIHPVIxI7JQ2pRCPJ1BCWkRnvvzp4rOsU4F/M7BVgGfDNrJ/ZbGZLgVuBllVdrwOuCdqj/m/+n8gM\nZy03s1eD7bzcvZ5MzeQl4DngHeCTYPc84NKgoL5P7jOIFIdWuRXpgJnVApe4++KE4+jj7o3BlcZD\nwB3u/lCSMUnl0ZWGSOmYFVwdrQT+BMxPOB6pQLrSEBGR0HSlISIioSlpiIhIaEoaIiISmpKGiIiE\npqQhIiKh/Q+fgQCyBxRqPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc80ced2940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "# setosa_x = iris.data[:50]\n",
    "# setosa_y = iris.target[:50]\n",
    "# versicolor_x = iris.data[50:100]\n",
    "# versicolor_y = iris.target[50:100]\n",
    "# scatter(setosa_x[:, 0], setosa_x[:, 2])\n",
    "# scatter(versicolor_x[:, 0], versicolor_x[:, 2])\n",
    "\n",
    "# Extract sepal length, petal length from Setosa and Versicolor\n",
    "data = iris.data[:100, [0, 2]]\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "# Split data to test and train data\n",
    "train_x, test_x, train_y, test_y = train_test_split(data, iris.target[:100], test_size=0.3)\n",
    "\n",
    "# Plotting data\n",
    "scatter(data[:50, 0], data[:50, 1], label='Setosa')\n",
    "scatter(data[50:100, 0], data[50:100, 1], label='Versicolour')\n",
    "title('Iris Data')\n",
    "xlabel('sepal length')\n",
    "ylabel('petal length')\n",
    "grid()\n",
    "legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with AdaDelta Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0] Accuracy: 0.0 \n",
      "[ 1] Accuracy: 0.0 \n",
      "[ 2] Accuracy: 0.0 \n",
      "[ 3] Accuracy: 0.0 \n",
      "[ 4] Accuracy: 0.0 \n",
      "[ 5] Accuracy: 0.0 \n",
      "[ 6] Accuracy: 0.07\n",
      "[ 7] Accuracy: 0.43\n",
      "[ 8] Accuracy: 0.5 \n",
      "[ 9] Accuracy: 0.5 \n",
      "[10] Accuracy: 0.5 \n",
      "[11] Accuracy: 0.5 \n",
      "[12] Accuracy: 0.5 \n",
      "[13] Accuracy: 0.57\n",
      "[14] Accuracy: 0.67\n",
      "[15] Accuracy: 0.7 \n",
      "[16] Accuracy: 0.73\n",
      "[17] Accuracy: 0.83\n",
      "[18] Accuracy: 0.83\n",
      "[19] Accuracy: 0.9 \n",
      "[20] Accuracy: 0.93\n",
      "[21] Accuracy: 0.97\n",
      "[22] Accuracy: 0.97\n",
      "[23] Accuracy: 0.97\n",
      "[24] Accuracy: 0.97\n",
      "[25] Accuracy: 0.97\n",
      "[26] Accuracy: 0.97\n",
      "[27] Accuracy: 0.97\n",
      "[28] Accuracy: 0.97\n",
      "[29] Accuracy: 0.97\n",
      "[30] Accuracy: 0.97\n",
      "[31] Accuracy: 0.97\n",
      "[32] Accuracy: 0.97\n",
      "[33] Accuracy: 0.97\n",
      "[34] Accuracy: 0.97\n",
      "[35] Accuracy: 0.97\n",
      "[36] Accuracy: 0.97\n",
      "[37] Accuracy: 0.97\n",
      "[38] Accuracy: 0.97\n",
      "[39] Accuracy: 0.97\n",
      "[40] Accuracy: 1.0 \n",
      "[41] Accuracy: 1.0 \n",
      "[42] Accuracy: 1.0 \n",
      "[43] Accuracy: 1.0 \n",
      "[44] Accuracy: 1.0 \n"
     ]
    }
   ],
   "source": [
    "w =   np.array([ 0.09370901, -0.24480254, -0.84210235]) # np.random.randn(2 + 1)\n",
    "\n",
    "def predict(w, x):\n",
    "    N = len(x)\n",
    "    yhat = w[1:].dot(x.T) + w[0]\n",
    "    return yhat\n",
    "\n",
    "def  adadelta_nn(w, X, Y, decay=0.2, epoch=4, weight_size=2):\n",
    "    \"\"\"\n",
    "    @param eta <float>: learning rate\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    e = 1e-8\n",
    "    \n",
    "    Eg = np.zeros(weight_size + 1)  # E[g^2]\n",
    "    Ed = np.zeros(weight_size + 1)  # E[\\Delta w^2]\n",
    "        \n",
    "    i = 0\n",
    "    for _ in range(epoch):\n",
    "        for _ in range(N):\n",
    "            x = X[i]\n",
    "            y = Y[i]\n",
    "            x = x.reshape((-1, 2))\n",
    "            yhat = predict(w, x)\n",
    "\n",
    "            # Calculate the gradients\n",
    "            gradient_w = 2/N*-(y-yhat).dot(x)\n",
    "            gradient_b = 2/N*-(y-yhat)\n",
    "\n",
    "            # Accumulate Gradient\n",
    "            Eg[1:] = decay * Eg[1:] + (1-decay) * gradient_w**2\n",
    "            Eg[0]  = decay * Eg[0] + (1-decay) * gradient_b**2\n",
    "\n",
    "            # Compute Update\n",
    "            delta_w = - np.sqrt(e + Ed[1:])/np.sqrt(e + Eg[1:]) * gradient_w\n",
    "            delta_b = - np.sqrt(e + Ed[0])/np.sqrt(e + Eg[0]) * gradient_b\n",
    "\n",
    "            # Accumulate Updates\n",
    "            Ed[1:] = decay * Ed[1:] + (1-decay) * delta_w**2\n",
    "            Ed[0]  = decay * Ed[0]  + (1-decay) * delta_b**2\n",
    "\n",
    "            w[1:] = w[1:] + delta_w\n",
    "            w[0] = w[0] + delta_b\n",
    "\n",
    "            i += 1\n",
    "            if i >= N:\n",
    "                i = 0\n",
    "           \n",
    "    return w\n",
    "\n",
    "\n",
    "for i in range(45):\n",
    "    w = adadelta_nn(w, train_x, train_y)\n",
    "\n",
    "    # Accuracy Test\n",
    "    yhats = predict(w, test_x)\n",
    "    yhats = np.where(yhats >= 0.5, 1, 0)\n",
    "    accuracy = round(accuracy_score(test_y, yhats), 2)\n",
    "    print(f'[{i:2}] Accuracy: {accuracy:<4.2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
