{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Single Neural Network (Perceptron)\n",
    "\n",
    "\n",
    "<img src=\"./images/single_layer.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "먼저 input의 weighted sum을 구합니다. <br>\n",
    "공식에 bias를 따로 $ b $로 잡았지만, 보통 weight의 첫번째 element는 bias로 사용합니다.\n",
    "\n",
    "\n",
    "$$ z = \\left[ \\sum^K_{i=1} w_i x_i \\right] + b = w^T x + b $$ \n",
    "\n",
    "\n",
    "** Derivative of the Weights **\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w} \\left[ w^T x + b \\right] = x$$\n",
    "\n",
    "\n",
    "** Derivative of the Bias **\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial b} \\left[ w^T x + b \\right] = 1$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_WEIGHT = 10\n",
    "w = np.random.randn(N_WEIGHT + 1) # + 1 is bias\n",
    "\n",
    "def cal_input(x):\n",
    "    return np.sum(w[1:].T * x + w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "$ \\phi $ 함수는 activation fuction을 나타내며 예제를 위해서 sigmoid function (or logistic function)을 사용하겠습니다.\n",
    "\n",
    "$$ \\phi(z; w) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "**Derivative of the sigmoid function**은 다음과 같습니다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{d}{dx} \\phi(z) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] & [1] \\\\\n",
    "&= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-z} \\right)^{-1}  & [2]\\\\\n",
    "&= -(1 + e^{-z})^{-2}(-e^{-z}) & [3]\\\\\n",
    "&= \\dfrac{e^{-x}}{\\left(1 + e^{-z}\\right)^2} & [4]\\\\\n",
    "&= \\dfrac{1}{1 + e^{-z}\\ } \\cdot \\dfrac{e^{-z}}{1 + e^{-x}}  & [5]\\\\\n",
    "&= \\dfrac{1}{1 + e^{-z}\\ } \\cdot \\dfrac{(1 + e^{-z}) - 1}{1 + e^{-z}}  & [6]\\\\\n",
    "&= \\dfrac{1}{1 + e^{-z}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-z}} \\right) & [7]\\\\\n",
    "&= \\phi(z) \\cdot (1 - \\phi(z)) & [8]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* [3] Chain Rule을 적용\n",
    "* [4] $ \\frac{d}{dx} e^{-z} = -e^{-z} $  이며  $ \\frac{d}{dx} e^{z} = e^{z} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    global w\n",
    "    return 1./(1+np.e**-z)\n",
    "\n",
    "def dsigmoid(y_pred):\n",
    "    return y_pred * (1. - y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function (Sum of squared Errors)\n",
    "\n",
    "먼저 예제로서 **Object function** $ J(w) $ (Sum of squared Errors - SSE) 를 정의합니다.<br>\n",
    "이때 $ \\phi(z^{(i)}) $ 는 activation function 입니다.\n",
    "\n",
    "$$ \\begin{align} \n",
    "J(w) &= \\frac{1}{N} \\sum_i \\left( y^{(i)} - \\phi(z^{(i)}) \\right)^2 \\\\\n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Gradient with regard to weights \n",
    "\n",
    "Optimization 문제는 objective function을 minimize 또는 maximize하는데 있습니다. <br>\n",
    "SSE를 사용시 minimize해야 하며, learning은  stochastic gradient descent를 통해서 처리를 하게 됩니다.\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w_i} = \n",
    "\\frac{\\partial J}{\\partial \\hat{y}} \\cdot \n",
    "\\frac{\\partial \\hat{y}}{\\partial z } \\cdot\n",
    "\\frac{\\partial z}{\\partial w_i } \n",
    "$$\n",
    "\n",
    "즉 다음과 같다고 할 수 있습니다. (sigmoid 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\begin{align} \n",
    "\\frac{\\partial J}{\\partial w_j} &= \\frac{\\partial}{\\partial w_j}  \\frac{1}{N} \\sum_i \\left(y^{(i)} - \\phi(z^{(i)}) \\right)^2 \\\\\n",
    "&= \\frac{2}{N} \\sum_i \\left( y^{(i)} - \\phi(z^{(i)}) \\right) \\frac{\\partial}{\\partial w_j} \\left(y^{(i)} - \\phi(z^{(i)}) \\right) \\\\\n",
    "&= \\frac{2}{N} \\sum_i \\left( y^{(i)} - \\phi(z^{(i)}) \\right) \\odot \\phi(z) \\cdot (1 - \\phi(z)) \\\\\n",
    "&= - \\frac{2}{N} \\sum_i \\left( y^{(i)} - \\phi(z^{(i)}) \\right) \\odot \\phi(z) \\cdot (1 - \\phi(z))\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Gradient with regard to bias \n",
    "\n",
    "$$ \\begin{align} \n",
    "\\frac{\\partial J}{\\partial b_j} &= \\frac{\\partial}{\\partial b_j}  \\frac{1}{N} \\sum_i \\left(y^{(i)} - \\phi(z^{(i)}) \\right)^2 \\\\\n",
    "&= \\frac{2}{N} \\sum_i \\left( y^{(i)} - \\phi(z^{(i)}) \\right) \\frac{\\partial}{\\partial b_j} \\left(y^{(i)} - \\phi(z^{(i)}) \\right) \\\\\n",
    "&= \\frac{2}{N} \\sum_i \\left( y^{(i)} - \\phi(z^{(i)}) \\right) \\frac{\\partial}{\\partial b_j} \\left[ y^{(i)} - \\sum_k \\left( w^{(i)}_k x^{(i)}_k + b^{i} \\right) \\right] \\\\\n",
    "&= \\frac{2}{N} \\sum_i \\left( y^{(i)} - \\phi(z^{(i)}) \\right)(0 - (0 + 1 ) ) \\\\\n",
    "&= - \\frac{2}{N} \\sum_i \\left( y^{(i)} - \\phi(z^{(i)}) \\right) \n",
    "\\end{align}$$\n",
    "\n",
    "### Update Weights\n",
    "\n",
    "$ \\eta $ 는 learning rate 입니다.\n",
    "\n",
    "$$ \\begin{align} \n",
    "\\Delta w &= - \\eta \\nabla J(w)  \\\\\n",
    "w &= w + \\Delta w\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "<img src=\"./images/neural_network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Algorithm\n",
    "\n",
    "* $ \\theta $ 는 neural network안의 모든 weights를 말합니다. \n",
    "* $ \\theta^{l}_{i, j} $ 는 l번째 weight를 가르킵니다.\n",
    "* layers의 인덱스는 1 (input), 2 (hidden), ... , L (output)을 가르킵니다.\n",
    "\n",
    "decision function $ h(x) $ 는 다음과 같이 설명될 수 있습니다.\n",
    "\n",
    "$$ \\begin{align}\n",
    "h^{(1)} &= x \\\\\n",
    "h^{(2)} &= g\\left( \\left( \\theta^{(1)} \\right)^T h^{(1)} + b^{(1)} \\right)\\\\\n",
    " ... \\\\\n",
    "h^{(L-1)} &= g\\left(  \\left( \\theta^{(L-2)} \\right)^T h^{(L-2)} + b^{(L-2)} \\right) \\\\\n",
    "h(x) = h^{(L)} &= g\\left( \\left( \\theta^{(L-1)} \\right)^T h^{(L-1)} + b^{(L-1)} \\right)\n",
    "\\end{align} $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, n_out, activation=None, batch_input_shape=None):\n",
    "        self.n_out = n_out\n",
    "        self.activation = activation\n",
    "        self.batch_input_shape = batch_input_shape\n",
    "        \n",
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.layers = list()\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    \n",
    "    \n",
    "model = Model()\n",
    "model.add(Layer(16, activation='sigmoid', batch_input_shape=(None, 9)))\n",
    "model.add(Layer(16, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "* https://cs.stanford.edu/~quocle/tutorial1.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
