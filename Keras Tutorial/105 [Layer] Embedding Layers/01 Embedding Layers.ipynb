{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Embedding Layers\n",
    "\n",
    "1. https://keras.io/layers/embeddings/\n",
    "2. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "### Embedding Layer\n",
    "\n",
    "* **input_dim**: size of the vocabulary (즉 sentence로 제공되는 vector안에서 가장 큰 integer값)\n",
    "* **output_dim**: dense embedding의 dimension\n",
    "* **mask_zero**: input value 0을 특수 padding으로서 masked out해야 될지 결정. recurrent layers사용시 variable length input을 사용시 유용하게 사용될 수 있으며, mask_zero True로 사용시 그 이후의 모든 layers들은 masking을 지원해야함. \n",
    "* **input_length**: input sequence가 constant값일때 사용. 특히 Flatten 사용후 Dense사용시 input_length값을 지정해줘야 Keras가 dense output의 shape을 알 수 있음\n",
    "\n",
    "### Input & Output Shape\n",
    "\n",
    "* Input Shape은 **(batch_size, sequence_length)** 입니다.\n",
    "* Output Shape은 **(batch_size, sequence_length, output_dim)** 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data\n",
    "\n",
    "데이터의 shape은 (batch, sentence vector)입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data1 = np.array([[0, 1, 2, 3], \n",
    "                  [0, 0, 0, 1], \n",
    "                  [9, 9, 9, 9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model\n",
    "\n",
    "Embedding(10, 1) 에서 input_dim을 10으로 주었습니다.<br>\n",
    "이는 데이터의 가장 큰 값이 9이며 zero-based index를 사용하기 때문에 9 + 1 = 10 이 되기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10, 1))\n",
    "model.compile('rmsprop', 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Result\n",
    "\n",
    "3번째 vector [9, 9, 9, 9]인데.. 이들의 값이 모두 동일하다는 것을 알 수 있습니다. <br>\n",
    "즉 어떤 단어의 integer값 (예를 들어 hello 는 5000)이라면 Embedding을 거치고 나면 5000이 아닌 0에서 1사이의 다른 값으로 동일하게 사용됩니다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01588077],\n",
       "        [ 0.04561056],\n",
       "        [ 0.02556488],\n",
       "        [ 0.04706056]],\n",
       "\n",
       "       [[ 0.01588077],\n",
       "        [ 0.01588077],\n",
       "        [ 0.01588077],\n",
       "        [ 0.04561056]],\n",
       "\n",
       "       [[-0.00760218],\n",
       "        [-0.00760218],\n",
       "        [-0.00760218],\n",
       "        [-0.00760218]]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
