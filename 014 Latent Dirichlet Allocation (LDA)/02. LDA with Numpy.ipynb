{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/anderson/.kaggle/kaggle.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/anderson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from pathlib import Path\n",
    "from tempfile import gettempdir\n",
    "\n",
    "import kaggle\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "kaggle.api.authenticate()\n",
    "nltk.download('wordnet')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_path = Path(gettempdir()) / 'news-headlines'\n",
    "data_name = 'therohk/million-headlines'\n",
    "kaggle.api.dataset_download_files(data_name, path=dest_path, unzip=True)\n",
    "\n",
    "data = pd.read_csv(dest_path / 'abcnews-date-text.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original  : ratepayers group wants compulsory local govt voting\n",
      "preprocess: ['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(lemmatizer.lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "doc_sample = data[data.index == 4310]['headline_text'].values[0]\n",
    "\n",
    "print('original  :', doc_sample)\n",
    "print('preprocess:', preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [decid, communiti, broadcast, licenc]\n",
       "1                        [wit, awar, defam]\n",
       "2    [call, infrastructur, protect, summit]\n",
       "3               [staff, aust, strike, rise]\n",
       "4      [strike, affect, australian, travel]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = data['headline_text'].apply(preprocess)\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n"
     ]
    }
   ],
   "source": [
    "def show_dict(data, n=10):\n",
    "    i = iter(data.iteritems())\n",
    "\n",
    "    for _ in range(10):\n",
    "        k, v = next(i)\n",
    "        print(k, v)\n",
    "\n",
    "# To Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "show_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34    : council         | count:1\n",
      "1216  : approv          | count:1\n",
      "1217  : farm            | count:1\n",
      "1218  : poultri         | count:1\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "# [[(index, 횟수), ....], ...]\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "bow_corpus[0]\n",
    "\n",
    "def display_bow(bow):\n",
    "    for idx, cnt in bow:\n",
    "        word = dictionary[idx]\n",
    "        print(f'{idx:<6}: {word:15} | count:{cnt}')\n",
    "    \n",
    "display_bow(bow_corpus[502])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_tfidf size: 1186018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.5850076620505259),\n",
       " (1, 0.38947256567331934),\n",
       " (2, 0.4997099083387053),\n",
       " (3, 0.5063271308533074)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "print('corpus_tfidf size:', len(corpus_tfidf))\n",
    "corpus_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
