{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "\n",
    "## Bellman Equation \n",
    "\n",
    "$$ Q(s, a) = learning\\ rate \\cdot (r + \\gamma( max(Q(s^{\\prime}, a^{\\prime})))) $$\n",
    "\n",
    "## Q Function\n",
    "\n",
    "$$ Q(s, a) = Q(s,a) + \\text{lr} \\left[ R(s, a) + \\gamma \\max Q^\\prime (s^\\prime, a^\\prime) - Q(s, a) \\right] $$\n",
    "\n",
    "* $ \\text{lr} $ : Learning rate\n",
    "* $ R(s, a) $ : 현재 state, action으로 얻은 reward\n",
    "* $ Q $ : 현재의 Q value\n",
    "* $ \\max Q^\\prime (s^\\prime, a^\\prime) $ : Maximum future reward\n",
    "* $ s^\\prime $ : step(action)으로 얻은 next_state\n",
    "* $ \\gamma $ : Discount rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: (19, 15, 3)\n",
      "Q Sample:\n",
      "[[[ 0.40362861 -0.33400827 -0.44276763]\n",
      "  [-0.20425145  0.44374132  0.35643621]\n",
      "  [ 0.6735468  -0.3671098   0.10382042]\n",
      "  [ 0.07533884 -0.9838463   0.23892995]\n",
      "  [-0.29104912  0.78122936 -0.84611013]\n",
      "  [ 0.69975269  0.88235987  0.12485457]\n",
      "  [ 0.22286157  0.54722626 -0.16735047]\n",
      "  [ 0.3656084   0.77050298 -0.19274283]\n",
      "  [-0.87375598  0.35647618  0.48079769]\n",
      "  [-0.84785423  0.04108182  0.30542148]\n",
      "  [ 0.05457983 -0.44679372 -0.60764636]\n",
      "  [ 0.52302156  0.87381095 -0.94724575]\n",
      "  [-0.52528738  0.77206436  0.68115652]\n",
      "  [-0.73443984  0.9866018  -0.45258224]\n",
      "  [ 0.47438185 -0.06215433  0.08366828]]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "n_state = (env.observation_space.high - env.observation_space.low) * np.array([10, 100])\n",
    "n_state = np.round(n_state, 0).astype(int) + 1\n",
    "\n",
    "Q = np.random.uniform(-1, 1, size=(n_state[0], n_state[1], env.action_space.n))\n",
    "print('Q shape:', Q.shape)\n",
    "print('Q Sample:')\n",
    "print(Q[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78201dd9ce8f4180b96bac1312f8fcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 | tot reward:-200.0 | epsilon:0.8999 | rand action:177 | Q action:23\n",
      "epoch:100 | tot reward:-200.0 | epsilon:0.8899 | rand action:179 | Q action:21\n",
      "epoch:200 | tot reward:-200.0 | epsilon:0.8799 | rand action:174 | Q action:26\n",
      "epoch:300 | tot reward:-200.0 | epsilon:0.8699 | rand action:183 | Q action:17\n",
      "epoch:400 | tot reward:-200.0 | epsilon:0.8599 | rand action:170 | Q action:30\n",
      "epoch:500 | tot reward:-200.0 | epsilon:0.8499 | rand action:163 | Q action:37\n",
      "epoch:600 | tot reward:-200.0 | epsilon:0.8399 | rand action:166 | Q action:34\n",
      "epoch:700 | tot reward:-200.0 | epsilon:0.8299 | rand action:163 | Q action:37\n",
      "epoch:800 | tot reward:-200.0 | epsilon:0.8199 | rand action:163 | Q action:37\n",
      "epoch:900 | tot reward:-200.0 | epsilon:0.8099 | rand action:167 | Q action:33\n",
      "epoch:1000 | tot reward:-200.0 | epsilon:0.7999 | rand action:149 | Q action:51\n",
      "epoch:1100 | tot reward:-200.0 | epsilon:0.7899 | rand action:168 | Q action:32\n",
      "epoch:1200 | tot reward:-200.0 | epsilon:0.7799 | rand action:164 | Q action:36\n",
      "epoch:1300 | tot reward:-200.0 | epsilon:0.7699 | rand action:147 | Q action:53\n",
      "epoch:1400 | tot reward:-200.0 | epsilon:0.7599 | rand action:153 | Q action:47\n",
      "epoch:1500 | tot reward:-200.0 | epsilon:0.7499 | rand action:138 | Q action:62\n",
      "epoch:1600 | tot reward:-200.0 | epsilon:0.7399 | rand action:145 | Q action:55\n",
      "epoch:1700 | tot reward:-200.0 | epsilon:0.7299 | rand action:151 | Q action:49\n",
      "epoch:1800 | tot reward:-200.0 | epsilon:0.7199 | rand action:123 | Q action:77\n",
      "epoch:1900 | tot reward:-200.0 | epsilon:0.7099 | rand action:137 | Q action:63\n",
      "epoch:2000 | tot reward:-200.0 | epsilon:0.6999 | rand action:131 | Q action:69\n",
      "epoch:2100 | tot reward:-200.0 | epsilon:0.6899 | rand action:135 | Q action:65\n",
      "epoch:2200 | tot reward:-200.0 | epsilon:0.6799 | rand action:138 | Q action:62\n",
      "epoch:2300 | tot reward:-200.0 | epsilon:0.6699 | rand action:127 | Q action:73\n",
      "epoch:2400 | tot reward:-200.0 | epsilon:0.6599 | rand action:140 | Q action:60\n",
      "epoch:2500 | tot reward:-200.0 | epsilon:0.6499 | rand action:130 | Q action:70\n",
      "epoch:2600 | tot reward:-200.0 | epsilon:0.6399 | rand action:135 | Q action:65\n",
      "epoch:2700 | tot reward:-200.0 | epsilon:0.6299 | rand action:124 | Q action:76\n",
      "epoch:2800 | tot reward:-200.0 | epsilon:0.6199 | rand action:132 | Q action:68\n",
      "epoch:2900 | tot reward:-200.0 | epsilon:0.6099 | rand action:125 | Q action:75\n",
      "epoch:3000 | tot reward:-200.0 | epsilon:0.5999 | rand action:118 | Q action:82\n",
      "epoch:3100 | tot reward:-200.0 | epsilon:0.5899 | rand action:118 | Q action:82\n",
      "epoch:3200 | tot reward:-200.0 | epsilon:0.5799 | rand action:117 | Q action:83\n",
      "epoch:3300 | tot reward:-200.0 | epsilon:0.5699 | rand action:110 | Q action:90\n",
      "epoch:3400 | tot reward:-200.0 | epsilon:0.5599 | rand action:114 | Q action:86\n",
      "epoch:3500 | tot reward:-200.0 | epsilon:0.5499 | rand action:114 | Q action:86\n",
      "epoch:3600 | tot reward:-200.0 | epsilon:0.5399 | rand action:119 | Q action:81\n",
      "epoch:3700 | tot reward:-200.0 | epsilon:0.5299 | rand action:104 | Q action:96\n",
      "epoch:3800 | tot reward:-200.0 | epsilon:0.5199 | rand action:102 | Q action:98\n",
      "epoch:3900 | tot reward:-200.0 | epsilon:0.5099 | rand action:98 | Q action:102\n",
      "epoch:4000 | tot reward:-200.0 | epsilon:0.4999 | rand action:98 | Q action:102\n",
      "epoch:4100 | tot reward:-200.0 | epsilon:0.4899 | rand action:93 | Q action:107\n",
      "epoch:4200 | tot reward:-200.0 | epsilon:0.4799 | rand action:102 | Q action:98\n",
      "epoch:4300 | tot reward:-200.0 | epsilon:0.4699 | rand action:83 | Q action:117\n",
      "epoch:4400 | tot reward:-200.0 | epsilon:0.4599 | rand action:92 | Q action:108\n",
      "epoch:4500 | tot reward:-200.0 | epsilon:0.4499 | rand action:84 | Q action:116\n",
      "epoch:4600 | tot reward:-200.0 | epsilon:0.4399 | rand action:84 | Q action:116\n",
      "epoch:4700 | tot reward:-200.0 | epsilon:0.4299 | rand action:84 | Q action:116\n",
      "epoch:4800 | tot reward:-200.0 | epsilon:0.4199 | rand action:73 | Q action:127\n",
      "epoch:4900 | tot reward:-200.0 | epsilon:0.4099 | rand action:103 | Q action:97\n",
      "epoch:5000 | tot reward:-200.0 | epsilon:0.3999 | rand action:94 | Q action:106\n",
      "epoch:5100 | tot reward:-200.0 | epsilon:0.3899 | rand action:68 | Q action:132\n",
      "epoch:5200 | tot reward:-200.0 | epsilon:0.3799 | rand action:74 | Q action:126\n",
      "epoch:5300 | tot reward:-200.0 | epsilon:0.3699 | rand action:78 | Q action:122\n",
      "epoch:5400 | tot reward:-200.0 | epsilon:0.3599 | rand action:59 | Q action:141\n",
      "epoch:5500 | tot reward:-200.0 | epsilon:0.3499 | rand action:63 | Q action:137\n",
      "epoch:5600 | tot reward:-200.0 | epsilon:0.3399 | rand action:75 | Q action:125\n",
      "epoch:5700 | tot reward:-200.0 | epsilon:0.3299 | rand action:61 | Q action:139\n",
      "epoch:5800 | tot reward:-200.0 | epsilon:0.3199 | rand action:70 | Q action:130\n",
      "epoch:5900 | tot reward:-200.0 | epsilon:0.3099 | rand action:59 | Q action:141\n",
      "epoch:6000 | tot reward:-200.0 | epsilon:0.2999 | rand action:56 | Q action:144\n",
      "epoch:6100 | tot reward:-200.0 | epsilon:0.2899 | rand action:57 | Q action:143\n",
      "epoch:6200 | tot reward:-189.0 | epsilon:0.2799 | rand action:43 | Q action:146\n",
      "epoch:6300 | tot reward:-200.0 | epsilon:0.2699 | rand action:55 | Q action:145\n",
      "epoch:6400 | tot reward:-200.0 | epsilon:0.2599 | rand action:53 | Q action:147\n",
      "epoch:6500 | tot reward:-200.0 | epsilon:0.2499 | rand action:51 | Q action:149\n",
      "epoch:6600 | tot reward:-200.0 | epsilon:0.2399 | rand action:54 | Q action:146\n",
      "epoch:6700 | tot reward:-200.0 | epsilon:0.2299 | rand action:49 | Q action:151\n",
      "epoch:6800 | tot reward:-156.0 | epsilon:0.2199 | rand action:19 | Q action:137\n",
      "epoch:6900 | tot reward:-200.0 | epsilon:0.2099 | rand action:48 | Q action:152\n",
      "epoch:7000 | tot reward:-200.0 | epsilon:0.1999 | rand action:39 | Q action:161\n",
      "epoch:7100 | tot reward:-155.0 | epsilon:0.1899 | rand action:28 | Q action:127\n",
      "epoch:7200 | tot reward:-163.0 | epsilon:0.1799 | rand action:26 | Q action:137\n",
      "epoch:7300 | tot reward:-200.0 | epsilon:0.1699 | rand action:41 | Q action:159\n",
      "epoch:7400 | tot reward:-159.0 | epsilon:0.1599 | rand action:25 | Q action:134\n",
      "epoch:7500 | tot reward:-166.0 | epsilon:0.1499 | rand action:20 | Q action:146\n",
      "epoch:7600 | tot reward:-193.0 | epsilon:0.1399 | rand action:32 | Q action:161\n",
      "epoch:7700 | tot reward:-200.0 | epsilon:0.1299 | rand action:36 | Q action:164\n",
      "epoch:7800 | tot reward:-151.0 | epsilon:0.1199 | rand action:19 | Q action:132\n",
      "epoch:7900 | tot reward:-200.0 | epsilon:0.1099 | rand action:20 | Q action:180\n",
      "epoch:8000 | tot reward:-200.0 | epsilon:0.0999 | rand action:25 | Q action:175\n",
      "epoch:8100 | tot reward:-157.0 | epsilon:0.0899 | rand action:14 | Q action:143\n",
      "epoch:8200 | tot reward:-174.0 | epsilon:0.0799 | rand action:15 | Q action:159\n",
      "epoch:8300 | tot reward:-200.0 | epsilon:0.0699 | rand action:13 | Q action:187\n",
      "epoch:8400 | tot reward:-200.0 | epsilon:0.0599 | rand action:16 | Q action:184\n",
      "epoch:8500 | tot reward:-200.0 | epsilon:0.0499 | rand action:8 | Q action:192\n",
      "epoch:8600 | tot reward:-162.0 | epsilon:0.0399 | rand action:7 | Q action:155\n",
      "epoch:8700 | tot reward:-153.0 | epsilon:0.0299 | rand action:2 | Q action:151\n",
      "epoch:8800 | tot reward:-159.0 | epsilon:0.0199 | rand action:2 | Q action:157\n",
      "epoch:8900 | tot reward:-200.0 | epsilon:0.0099 | rand action:1 | Q action:199\n",
      "epoch:9000 | tot reward:-167.0 | epsilon:0.0 | rand action:0 | Q action:167\n",
      "epoch:9100 | tot reward:-187.0 | epsilon:0.0 | rand action:0 | Q action:187\n",
      "epoch:9200 | tot reward:-144.0 | epsilon:0.0 | rand action:0 | Q action:144\n",
      "epoch:9300 | tot reward:-128.0 | epsilon:0.0 | rand action:0 | Q action:128\n",
      "epoch:9400 | tot reward:-154.0 | epsilon:0.0 | rand action:0 | Q action:154\n",
      "epoch:9500 | tot reward:-151.0 | epsilon:0.0 | rand action:0 | Q action:151\n",
      "epoch:9600 | tot reward:-148.0 | epsilon:0.0 | rand action:0 | Q action:148\n",
      "epoch:9700 | tot reward:-134.0 | epsilon:0.0 | rand action:0 | Q action:134\n",
      "epoch:9800 | tot reward:-134.0 | epsilon:0.0 | rand action:0 | Q action:134\n",
      "epoch:9900 | tot reward:-143.0 | epsilon:0.0 | rand action:0 | Q action:143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def discretize(env, state):\n",
    "    state = (state - env.observation_space.low) * np.array([10, 100])\n",
    "    state = np.round(state, 0).astype(int)\n",
    "    return state\n",
    "\n",
    "def train(env, Q, epochs=10000, lr=0.1, gamma=0.9, epsilon=0.9):\n",
    "    np.random.seed(2424)\n",
    "    reduction = epsilon/epochs\n",
    "    action_n = env.action_space.n\n",
    "    \n",
    "    rewards = list()\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        state = env.reset()\n",
    "        state = discretize(env, state)\n",
    "        \n",
    "        done = False\n",
    "        _tot_reward = 0\n",
    "        _tot_rand_action = 0\n",
    "        _tot_q_action = 0\n",
    "        _max_pos = 0\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            # Calculate next action\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state[0], state[1]])\n",
    "                _tot_q_action += 1\n",
    "            else:\n",
    "                action = np.random.randint(0, action_n)\n",
    "                _tot_rand_action += 1\n",
    "                \n",
    "            # Step!\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state_apx = discretize(env, next_state)\n",
    "\n",
    "            # Terminal Update\n",
    "            if done and next_state[0] >= 0.5:\n",
    "                Q[next_state_apx[0], next_state_apx[1], action] = reward\n",
    "            else:\n",
    "                delta = lr * (reward + gamma * np.max(Q[next_state_apx[0], next_state_apx[1]]) - \n",
    "                              Q[state[0], state[1], action])\n",
    "                Q[state[0], state[1], action] += delta\n",
    "            \n",
    "            state = next_state_apx\n",
    "            _tot_reward += reward\n",
    "            \n",
    "        # Decay Epsilon\n",
    "        if epsilon > 0:\n",
    "            epsilon -= reduction\n",
    "            epsilon = round(epsilon, 4)\n",
    "            \n",
    "        # Track Rewards\n",
    "        rewards.append(_tot_reward)\n",
    "        \n",
    "        # Log\n",
    "        if epoch%100 == 0:\n",
    "            print(f'\\repoch:{epoch} | tot reward:{_tot_reward} | epsilon:{epsilon} | ' \n",
    "                  f'rand action:{_tot_rand_action} | Q action:{_tot_q_action}')\n",
    "\n",
    "train(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 5] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 5] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 5] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 5] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[12  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  6] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  6] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  6] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  4] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  4] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  4] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[3 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[3 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 5] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 5] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 1 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 2 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 2 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 2 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 3 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 3 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 3 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 4 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 4 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 5 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 5 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[12 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[12 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[13 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[13 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[14 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[14 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[14 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[17 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[17 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[17 10] | reward:-1.0 | done:True | info:{}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "state = discretize(env, state)\n",
    "\n",
    "env.render()\n",
    "input()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    action = np.argmax(Q[state[0], state[1]])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    state = discretize(env, state)\n",
    "    \n",
    "    print(f'\\rstate:{state} | reward:{reward} | done:{done} | info:{info}')\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Play "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "state:[-0.43620921 -0.00165996] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.43851711 -0.00230789] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.44145621 -0.0029391 ] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.44500516 -0.00354895] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.44913812 -0.00413296] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.4538249  -0.00468678] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.45903118 -0.00520627] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.46471868 -0.00568751] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.4708455  -0.00612681] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.47636631 -0.00552081] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.48124019 -0.00487387] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.4854309  -0.00419071] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.48890724 -0.00347634] | reward:-1.0 | done:False | info:{}\n",
      "22\n",
      "2\n",
      "state:[-0.4916433  -0.00273606] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.49361866 -0.00197536] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.49481857 -0.00119991] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-4.95234058e-01 -4.15490742e-04] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-4.94862027e-01  3.72030517e-04] | reward:-1.0 | done:False | info:{}\n",
      "3\n",
      "3\n",
      "2\n",
      "state:[-0.49370526  0.00115677] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.49277238  0.00093287] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.49207038  0.000702  ] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-4.91604491e-01  4.65891731e-04] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-4.91378188e-01  2.26302867e-04] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-4.91393163e-01 -1.49754013e-05] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-4.91649305e-01 -2.56141878e-04] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.4921447 -0.0004954] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.49287565 -0.00073095] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-4.92836702e-01  3.89508906e-05] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.49202814  0.00080856] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.490456    0.00157214] | reward:-1.0 | done:False | info:{}\n",
      "290\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "0\n",
      "state:[-4.90132027e-01  3.23975323e-04] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.49105863 -0.0009266 ] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.4932289  -0.00217027] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.49662663 -0.00339773] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.50122642 -0.0045998 ] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.50699389 -0.00576746] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.51388583 -0.00689195] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.52185062 -0.00796479] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.53082852 -0.0089779 ] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.53975219 -0.00892368] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.54855477 -0.00880257] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.55717035 -0.00861558] | reward:-1.0 | done:False | info:{}\n",
      "1\n",
      "state:[-0.56553457 -0.00836422] | reward:-1.0 | done:False | info:{}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2eea1ff6e340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    while True:\n",
    "        action = int(input())\n",
    "        if action in [0, 1, 2]:\n",
    "            break\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f'\\rstate:{state} | reward:{reward} | done:{done} | info:{info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MP4 to GIf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.video.fx import rotate\n",
    "\n",
    "clip = VideoFileClip('mountain-car.mp4', audio=False)\n",
    "clip = clip\n",
    "clip.write_gif('mountain-car.gif', fps=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
