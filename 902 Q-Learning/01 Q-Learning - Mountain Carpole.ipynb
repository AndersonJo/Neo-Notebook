{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "\n",
    "## Bellman Equation \n",
    "\n",
    "$$ Q(s, a) = learning\\ rate \\cdot (r + \\gamma( max(Q(s^{\\prime}, a^{\\prime})))) $$\n",
    "\n",
    "## Q Function\n",
    "\n",
    "$$ Q(s, a) = Q(s,a) + \\text{lr} \\left[ R(s, a) + \\gamma \\max Q^\\prime (s^\\prime, a^\\prime) - Q(s, a) \\right] $$\n",
    "\n",
    "* $ \\text{lr} $ : Learning rate\n",
    "* $ R(s, a) $ : 현재 state, action으로 얻은 reward\n",
    "* $ Q $ : 현재의 Q value\n",
    "* $ \\max Q^\\prime (s^\\prime, a^\\prime) $ : Maximum future reward\n",
    "* $ s^\\prime $ : step(action)으로 얻은 next_state\n",
    "* $ \\gamma $ : Discount rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: (19, 15, 3)\n",
      "Q Sample:\n",
      "[[[ 0.82619025 -0.06216362 -0.68491643]\n",
      "  [-0.46537736  0.0026488   0.76352481]\n",
      "  [-0.46084451  0.41834014 -0.70205364]\n",
      "  [-0.50025345 -0.99267844  0.44491119]\n",
      "  [ 0.52070402 -0.08365642  0.97717661]\n",
      "  [-0.46801954  0.92435095  0.63541954]\n",
      "  [ 0.57558932 -0.9850298   0.42748597]\n",
      "  [ 0.7221162  -0.21099647  0.41876368]\n",
      "  [ 0.67178309  0.32167557 -0.48464403]\n",
      "  [-0.09345879 -0.39326889 -0.69800853]\n",
      "  [ 0.63033553  0.79487805 -0.88664955]\n",
      "  [ 0.13433765  0.87601109  0.6994363 ]\n",
      "  [ 0.33959778  0.29759607 -0.22626105]\n",
      "  [ 0.36603566  0.11900018 -0.38516679]\n",
      "  [ 0.5702685   0.66159952  0.04293651]]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "n_state = (env.observation_space.high - env.observation_space.low) * np.array([10, 100])\n",
    "n_state = np.round(n_state, 0).astype(int) + 1\n",
    "\n",
    "Q = np.random.uniform(-1, 1, size=(n_state[0], n_state[1], env.action_space.n))\n",
    "print('Q shape:', Q.shape)\n",
    "print('Q Sample:')\n",
    "print(Q[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87da425baead4883b1af2345ecd4a8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 | tot reward:-200.0 | epsilon:0.8999 | rand action:183 | Q action:17\n",
      "epoch:100 | tot reward:-200.0 | epsilon:0.8899 | rand action:184 | Q action:16\n",
      "epoch:200 | tot reward:-200.0 | epsilon:0.8799 | rand action:177 | Q action:23\n",
      "epoch:300 | tot reward:-200.0 | epsilon:0.8699 | rand action:174 | Q action:26\n",
      "epoch:400 | tot reward:-200.0 | epsilon:0.8599 | rand action:166 | Q action:34\n",
      "epoch:500 | tot reward:-200.0 | epsilon:0.8499 | rand action:163 | Q action:37\n",
      "epoch:600 | tot reward:-200.0 | epsilon:0.8399 | rand action:172 | Q action:28\n",
      "epoch:700 | tot reward:-200.0 | epsilon:0.8299 | rand action:167 | Q action:33\n",
      "epoch:800 | tot reward:-200.0 | epsilon:0.8199 | rand action:159 | Q action:41\n",
      "epoch:900 | tot reward:-200.0 | epsilon:0.8099 | rand action:158 | Q action:42\n",
      "epoch:1000 | tot reward:-200.0 | epsilon:0.7999 | rand action:164 | Q action:36\n",
      "epoch:1100 | tot reward:-200.0 | epsilon:0.7899 | rand action:156 | Q action:44\n",
      "epoch:1200 | tot reward:-200.0 | epsilon:0.7799 | rand action:160 | Q action:40\n",
      "epoch:1300 | tot reward:-200.0 | epsilon:0.7699 | rand action:156 | Q action:44\n",
      "epoch:1400 | tot reward:-200.0 | epsilon:0.7599 | rand action:153 | Q action:47\n",
      "epoch:1500 | tot reward:-200.0 | epsilon:0.7499 | rand action:147 | Q action:53\n",
      "epoch:1600 | tot reward:-200.0 | epsilon:0.7399 | rand action:158 | Q action:42\n",
      "epoch:1700 | tot reward:-200.0 | epsilon:0.7299 | rand action:149 | Q action:51\n",
      "epoch:1800 | tot reward:-200.0 | epsilon:0.7199 | rand action:139 | Q action:61\n",
      "epoch:1900 | tot reward:-200.0 | epsilon:0.7099 | rand action:145 | Q action:55\n",
      "epoch:2000 | tot reward:-200.0 | epsilon:0.6999 | rand action:133 | Q action:67\n",
      "epoch:2100 | tot reward:-200.0 | epsilon:0.6899 | rand action:153 | Q action:47\n",
      "epoch:2200 | tot reward:-200.0 | epsilon:0.6799 | rand action:138 | Q action:62\n",
      "epoch:2300 | tot reward:-200.0 | epsilon:0.6699 | rand action:126 | Q action:74\n",
      "epoch:2400 | tot reward:-200.0 | epsilon:0.6599 | rand action:135 | Q action:65\n",
      "epoch:2500 | tot reward:-200.0 | epsilon:0.6499 | rand action:135 | Q action:65\n",
      "epoch:2600 | tot reward:-200.0 | epsilon:0.6399 | rand action:126 | Q action:74\n",
      "epoch:2700 | tot reward:-200.0 | epsilon:0.6299 | rand action:123 | Q action:77\n",
      "epoch:2800 | tot reward:-200.0 | epsilon:0.6199 | rand action:123 | Q action:77\n",
      "epoch:2900 | tot reward:-200.0 | epsilon:0.6099 | rand action:123 | Q action:77\n",
      "epoch:3000 | tot reward:-200.0 | epsilon:0.5999 | rand action:117 | Q action:83\n",
      "epoch:3100 | tot reward:-200.0 | epsilon:0.5899 | rand action:134 | Q action:66\n",
      "epoch:3200 | tot reward:-200.0 | epsilon:0.5799 | rand action:119 | Q action:81\n",
      "epoch:3300 | tot reward:-200.0 | epsilon:0.5699 | rand action:120 | Q action:80\n",
      "epoch:3400 | tot reward:-200.0 | epsilon:0.5599 | rand action:117 | Q action:83\n",
      "epoch:3500 | tot reward:-200.0 | epsilon:0.5499 | rand action:105 | Q action:95\n",
      "epoch:3600 | tot reward:-200.0 | epsilon:0.5399 | rand action:113 | Q action:87\n",
      "epoch:3700 | tot reward:-200.0 | epsilon:0.5299 | rand action:118 | Q action:82\n",
      "epoch:3800 | tot reward:-200.0 | epsilon:0.5199 | rand action:91 | Q action:109\n",
      "epoch:3900 | tot reward:-200.0 | epsilon:0.5099 | rand action:102 | Q action:98\n",
      "epoch:4000 | tot reward:-200.0 | epsilon:0.4999 | rand action:90 | Q action:110\n",
      "epoch:4100 | tot reward:-200.0 | epsilon:0.4899 | rand action:102 | Q action:98\n",
      "epoch:4200 | tot reward:-200.0 | epsilon:0.4799 | rand action:96 | Q action:104\n",
      "epoch:4300 | tot reward:-200.0 | epsilon:0.4699 | rand action:95 | Q action:105\n",
      "epoch:4400 | tot reward:-200.0 | epsilon:0.4599 | rand action:87 | Q action:113\n",
      "epoch:4500 | tot reward:-200.0 | epsilon:0.4499 | rand action:87 | Q action:113\n",
      "epoch:4600 | tot reward:-200.0 | epsilon:0.4399 | rand action:79 | Q action:121\n",
      "epoch:4700 | tot reward:-200.0 | epsilon:0.4299 | rand action:85 | Q action:115\n",
      "epoch:4800 | tot reward:-200.0 | epsilon:0.4199 | rand action:81 | Q action:119\n",
      "epoch:4900 | tot reward:-200.0 | epsilon:0.4099 | rand action:76 | Q action:124\n",
      "epoch:5000 | tot reward:-200.0 | epsilon:0.3999 | rand action:75 | Q action:125\n",
      "epoch:5100 | tot reward:-200.0 | epsilon:0.3899 | rand action:66 | Q action:134\n",
      "epoch:5200 | tot reward:-200.0 | epsilon:0.3799 | rand action:85 | Q action:115\n",
      "epoch:5300 | tot reward:-200.0 | epsilon:0.3699 | rand action:78 | Q action:122\n",
      "epoch:5400 | tot reward:-200.0 | epsilon:0.3599 | rand action:77 | Q action:123\n",
      "epoch:5500 | tot reward:-200.0 | epsilon:0.3499 | rand action:74 | Q action:126\n",
      "epoch:5600 | tot reward:-200.0 | epsilon:0.3399 | rand action:68 | Q action:132\n",
      "epoch:5700 | tot reward:-200.0 | epsilon:0.3299 | rand action:45 | Q action:155\n",
      "epoch:5800 | tot reward:-159.0 | epsilon:0.3199 | rand action:53 | Q action:108\n",
      "epoch:5900 | tot reward:-200.0 | epsilon:0.3099 | rand action:63 | Q action:137\n",
      "epoch:6000 | tot reward:-200.0 | epsilon:0.2999 | rand action:61 | Q action:139\n",
      "epoch:6100 | tot reward:-200.0 | epsilon:0.2899 | rand action:53 | Q action:147\n",
      "epoch:6200 | tot reward:-200.0 | epsilon:0.2799 | rand action:54 | Q action:146\n",
      "epoch:6300 | tot reward:-200.0 | epsilon:0.2699 | rand action:55 | Q action:145\n",
      "epoch:6400 | tot reward:-200.0 | epsilon:0.2599 | rand action:47 | Q action:153\n",
      "epoch:6500 | tot reward:-186.0 | epsilon:0.2499 | rand action:46 | Q action:142\n",
      "epoch:6600 | tot reward:-200.0 | epsilon:0.2399 | rand action:49 | Q action:151\n",
      "epoch:6700 | tot reward:-200.0 | epsilon:0.2299 | rand action:53 | Q action:147\n",
      "epoch:6800 | tot reward:-183.0 | epsilon:0.2199 | rand action:38 | Q action:147\n",
      "epoch:6900 | tot reward:-200.0 | epsilon:0.2099 | rand action:36 | Q action:164\n",
      "epoch:7000 | tot reward:-200.0 | epsilon:0.1999 | rand action:35 | Q action:165\n",
      "epoch:7100 | tot reward:-200.0 | epsilon:0.1899 | rand action:24 | Q action:176\n",
      "epoch:7200 | tot reward:-200.0 | epsilon:0.1799 | rand action:41 | Q action:159\n",
      "epoch:7300 | tot reward:-178.0 | epsilon:0.1699 | rand action:27 | Q action:153\n",
      "epoch:7400 | tot reward:-200.0 | epsilon:0.1599 | rand action:35 | Q action:165\n",
      "epoch:7500 | tot reward:-182.0 | epsilon:0.1499 | rand action:29 | Q action:155\n",
      "epoch:7600 | tot reward:-200.0 | epsilon:0.1399 | rand action:24 | Q action:176\n",
      "epoch:7700 | tot reward:-200.0 | epsilon:0.1299 | rand action:30 | Q action:170\n",
      "epoch:7800 | tot reward:-151.0 | epsilon:0.1199 | rand action:19 | Q action:134\n",
      "epoch:7900 | tot reward:-200.0 | epsilon:0.1099 | rand action:28 | Q action:172\n",
      "epoch:8000 | tot reward:-200.0 | epsilon:0.0999 | rand action:20 | Q action:180\n",
      "epoch:8100 | tot reward:-200.0 | epsilon:0.0899 | rand action:17 | Q action:183\n",
      "epoch:8200 | tot reward:-145.0 | epsilon:0.0799 | rand action:13 | Q action:134\n",
      "epoch:8300 | tot reward:-144.0 | epsilon:0.0699 | rand action:8 | Q action:138\n",
      "epoch:8400 | tot reward:-200.0 | epsilon:0.0599 | rand action:20 | Q action:180\n",
      "epoch:8500 | tot reward:-170.0 | epsilon:0.0499 | rand action:7 | Q action:165\n",
      "epoch:8600 | tot reward:-157.0 | epsilon:0.0399 | rand action:4 | Q action:155\n",
      "epoch:8700 | tot reward:-200.0 | epsilon:0.0299 | rand action:7 | Q action:193\n",
      "epoch:8800 | tot reward:-144.0 | epsilon:0.0199 | rand action:2 | Q action:144\n",
      "epoch:8900 | tot reward:-142.0 | epsilon:0.0099 | rand action:0 | Q action:144\n",
      "epoch:9000 | tot reward:-145.0 | epsilon:0.0 | rand action:0 | Q action:147\n",
      "epoch:9100 | tot reward:-139.0 | epsilon:0.0 | rand action:0 | Q action:141\n",
      "epoch:9200 | tot reward:-138.0 | epsilon:0.0 | rand action:0 | Q action:140\n",
      "epoch:9300 | tot reward:-131.0 | epsilon:0.0 | rand action:0 | Q action:133\n",
      "epoch:9400 | tot reward:-149.0 | epsilon:0.0 | rand action:0 | Q action:151\n",
      "epoch:9500 | tot reward:-139.0 | epsilon:0.0 | rand action:0 | Q action:141\n",
      "epoch:9600 | tot reward:-133.0 | epsilon:0.0 | rand action:0 | Q action:135\n",
      "epoch:9700 | tot reward:-151.0 | epsilon:0.0 | rand action:0 | Q action:153\n",
      "epoch:9800 | tot reward:-155.0 | epsilon:0.0 | rand action:0 | Q action:157\n",
      "epoch:9900 | tot reward:-158.0 | epsilon:0.0 | rand action:0 | Q action:160\n"
     ]
    }
   ],
   "source": [
    "def discretize(env, state):\n",
    "    state = (state - env.observation_space.low) * np.array([10, 100])\n",
    "    state = np.round(state, 0).astype(int)\n",
    "    return state\n",
    "\n",
    "def train(env, Q, epochs=10000, lr=0.1, gamma=0.9, epsilon=0.9):\n",
    "    reduction = epsilon/epochs\n",
    "    action_n = env.action_space.n\n",
    "    \n",
    "    rewards = list()\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        state = env.reset()\n",
    "        state = discretize(env, state)\n",
    "        \n",
    "        done = False\n",
    "        _tot_reward = 0\n",
    "        _tot_rand_action = 0\n",
    "        _tot_q_action = 0\n",
    "        _max_pos = 0\n",
    "        \n",
    "        while not done:\n",
    "#             env.render()\n",
    "\n",
    "            # Calculate next action\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state[0], state[1]])\n",
    "                _tot_q_action += 1\n",
    "            else:\n",
    "                action = np.random.randint(0, action_n)\n",
    "                _tot_rand_action += 1\n",
    "                \n",
    "            # Step!\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state_apx = discretize(env, next_state)\n",
    "\n",
    "            # Terminal Update\n",
    "            if done and next_state[0] >= 0.5:\n",
    "                reward = 1\n",
    "                Q[next_state_apx[0], next_state_apx[1], action] = reward\n",
    "            else:\n",
    "                delta = lr * (reward + gamma * np.max(Q[next_state_apx[0], next_state_apx[1]]) - \n",
    "                              Q[state[0], state[1], action])\n",
    "                Q[state[0], state[1], action] += delta\n",
    "            \n",
    "            state = next_state_apx\n",
    "            _tot_reward += reward\n",
    "            \n",
    "        # Decay Epsilon\n",
    "        if epsilon > 0:\n",
    "            epsilon -= reduction\n",
    "            epsilon = round(epsilon, 4)\n",
    "            \n",
    "        # Track Rewards\n",
    "        rewards.append(_tot_reward)\n",
    "        \n",
    "        # Log\n",
    "        if epoch%100 == 0:\n",
    "            print(f'\\repoch:{epoch} | tot reward:{_tot_reward} | epsilon:{epsilon} | ' \n",
    "                  f'rand action:{_tot_rand_action} | Q action:{_tot_q_action}')\n",
    "\n",
    "train(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 6] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 5 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[12  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[12  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[12  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[12  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[12  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  9] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  8] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  7] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  6] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  6] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  6] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[13  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[12  5] | reward:-1.0 | done:False | info:{}\n",
      "state:[12  4] | reward:-1.0 | done:False | info:{}\n",
      "state:[12  4] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  4] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  3] | reward:-1.0 | done:False | info:{}\n",
      "state:[11  3] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  3] | reward:-1.0 | done:False | info:{}\n",
      "state:[10  3] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[9 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[8 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[7 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[6 1] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 1] | reward:-1.0 | done:False | info:{}\n",
      "state:[5 1] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 1] | reward:-1.0 | done:False | info:{}\n",
      "state:[4 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[3 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[3 2] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[2 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 3] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 4] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 7] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 8] | reward:-1.0 | done:False | info:{}\n",
      "state:[0 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[1 9] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 1 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 1 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 2 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 2 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 2 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 3 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 3 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 4 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 4 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 5 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 5 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 6 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 7 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 13] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 8 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[ 9 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[10 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[11 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[12 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[12 12] | reward:-1.0 | done:False | info:{}\n",
      "state:[13 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[13 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[13 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[14 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[14 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 11] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[15 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[16 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[17 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[17 10] | reward:-1.0 | done:False | info:{}\n",
      "state:[17 10] | reward:-1.0 | done:True | info:{}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "state = discretize(env, state)\n",
    "\n",
    "env.render()\n",
    "input()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    action = np.argmax(Q[state[0], state[1]])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    state = discretize(env, state)\n",
    "    \n",
    "    print(f'\\rstate:{state} | reward:{reward} | done:{done} | info:{info}')\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Play "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "state:[-5.96023268e-01 -4.64482215e-04] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.59694883 -0.00092556] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.5983287  -0.00137987] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.60015277 -0.00182407] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.60240772 -0.00225495] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.6050771  -0.00266938] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.60814146 -0.00306436] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.61157852 -0.00343706] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.61536337 -0.00378485] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.61946865 -0.00410528] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.62386477 -0.00439612] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.62852019 -0.00465542] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.63340163 -0.00488144] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.63847435 -0.00507273] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.64370247 -0.00522811] | reward:-1.0 | done:False | info:{}\n",
      "02\n",
      "state:[-0.64704917 -0.0033467 ] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.64849101 -0.00144184] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-6.48017929e-01  4.73085030e-04] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.64563322  0.00238471] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.64135356  0.00427966] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.63520899  0.00614457] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.6272429   0.00796609] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.61751194  0.00973096] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.60608592  0.01142602] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.59304754  0.01303838] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.57849205  0.01455549] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.56252672  0.01596533] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.5452701   0.01725663] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.52685105  0.01841905] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.50740761  0.01944344] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.48708556  0.02032205] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.46603681  0.02104875] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.44441764  0.02161918] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.42238675  0.02203089] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.40010334  0.02228341] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.3777251   0.02237824] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.35540633  0.02231877] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.33329615  0.02211018] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.31153697  0.02175919] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.29026311  0.02127386] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.26959981  0.0206633 ] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.24966242  0.01993738] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.23055599  0.01910644] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.21237501  0.01818098] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.19520356  0.01717145] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.17911554  0.01608802] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.1641752   0.01494034] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.15043771  0.01373749] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.13794991  0.0124878 ] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.12675105  0.01119885] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.11687363  0.00987743] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.1083441   0.00852953] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.10118368  0.00716043] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.09540895  0.00577472] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.09103252  0.00437643] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.08806344  0.00296908] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.08650762  0.00155582] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.08636808  0.00013954] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.08964509 -0.00327701] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.09633223 -0.00668714] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.1064157  -0.01008347] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.11987285 -0.01345715] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.13667008 -0.01679723] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.1567601  -0.02009002] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.18007872 -0.02331862] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.20654131 -0.02646259] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.23603915 -0.02949783] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.26843595 -0.0323968 ] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.30356498 -0.03512903] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.341227   -0.03766202] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.38118958 -0.03996258] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.42318804 -0.04199846] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.46692823 -0.0437402 ] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.51209142 -0.04516318] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.55834089 -0.04624947] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.60533027 -0.04698938] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.65271278 -0.04738251] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.70015097 -0.04743819] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.74732607 -0.0471751 ] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.7939464  -0.04662032] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.8397542  -0.04580781] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.8845307 -0.0447765] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.92809902 -0.04356832] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-0.97032523 -0.0422262 ] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-1.01111762 -0.04079239] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-1.05042464 -0.03930702] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-1.08823178 -0.03780714] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-1.12455783 -0.03632606] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-1.15945092 -0.03489308] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-1.19298442 -0.03353351] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-1.2  0. ] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-1.1987581  0.0012419] | reward:-1.0 | done:False | info:{}\n",
      "0\n",
      "state:[-1.19627021  0.0024879 ] | reward:-1.0 | done:False | info:{}\n",
      "02\n",
      "state:[-1.19052817  0.00574203] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-1.18151372  0.00901446] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-1.16919949  0.01231423] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-1.15355085  0.01564864] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-1.13452839  0.01902246] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-1.11209123  0.02243715] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-1.08620131  0.02588993] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-1.05682847  0.02937283] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-1.02395668  0.03287179] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.98759097  0.03636572] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.94776512  0.03982585] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.90454967  0.04321545] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.85805967  0.04649   ] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.80846144  0.04959823] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.75597745  0.05248399] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.7008884   0.05508905] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.64353149  0.05735691] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.58429437  0.05923712] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.52360454  0.06068983] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.46191467  0.06168987] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.3996848   0.06222988] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.33736302  0.06232178] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.27536646  0.06199655] | reward:-1.0 | done:False | info:{}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "state:[-0.21406428  0.06130218] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.15376406  0.06030022] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.09470254  0.05906152] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[-0.0370408   0.05766174] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.01913637 0.05617716] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.07381765 0.05468128] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.12705998 0.05324233] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.17898175 0.05192177] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.22975533 0.05077358] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.27959963 0.0498443 ] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.32877303 0.04917341] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.37756703 0.048794  ] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.42630049 0.04873346] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.47531453 0.04901404] | reward:-1.0 | done:False | info:{}\n",
      "2\n",
      "state:[0.5249677  0.04965317] | reward:-1.0 | done:True | info:{}\n",
      "2\n",
      "state:[0.57563114 0.05066344] | reward:-1.0 | done:True | info:{}\n",
      "2\n",
      "state:[0.6       0.0520521] | reward:-1.0 | done:True | info:{}\n",
      "2\n",
      "state:[0.6       0.0536201] | reward:-1.0 | done:True | info:{}\n",
      "2\n",
      "state:[0.6        0.05518811] | reward:-1.0 | done:True | info:{}\n",
      "2\n",
      "state:[0.6        0.05675612] | reward:-1.0 | done:True | info:{}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2eea1ff6e340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    while True:\n",
    "        action = int(input())\n",
    "        if action in [0, 1, 2]:\n",
    "            break\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f'\\rstate:{state} | reward:{reward} | done:{done} | info:{info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f1f32cb6c343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MountainCar-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f1f32cb6c343>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, epoch, lr, gamma)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "class QLearning(object):\n",
    "    \n",
    "    def __init__(self, game, epoch = 5000, lr=0.9, gamma=0.95):\n",
    "        self.env = gym.make(game)\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.Q = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        \n",
    "    def train(self):\n",
    "        env, Q, lr, gamma = self.env, self.Q, self.lr, self.gamma\n",
    "        for i in range(1, self.epoch+1):\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            while True:\n",
    "                action = np.argmax(Q[state,:] + np.random.randn(1, env.action_space.n)*(1./(i+1)))\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                Q[state, action] += lr*(reward + gamma * max(Q[next_state,:]) - Q[state, action])\n",
    "                print(state)\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "                state = next_state\n",
    "            \n",
    "            if i% 1000 == 0:\n",
    "                self.render()\n",
    "#                 print('epoch:', i\n",
    "#                 print Q\n",
    "    \n",
    "    def play(self, render=True):\n",
    "        env, Q = self.env, self.Q\n",
    "        score = 0\n",
    "        count = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) * 0.01)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if render:\n",
    "                self.render()\n",
    "                \n",
    "            count += 1\n",
    "            score += reward\n",
    "            if done:\n",
    "                self.render()\n",
    "                break\n",
    "        print('Total Score:', score, 'Move Count:', count)\n",
    "        return score, count\n",
    "    \n",
    "    def render(self):\n",
    "        clear_output(True)\n",
    "        self.env.render()\n",
    "        sleep(0.05)\n",
    "\n",
    "q = QLearning('MountainCar-v0')\n",
    "q.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
