{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnn_with_numpy' from 'rnn_with_numpy.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import rnn_with_numpy as rnn\n",
    "reload(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anderson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_START = '_SETENCE_START'\n",
    "SENTENCE_END = '_SENTENCE_END'\n",
    "UNKNOWN_TOKEN = '_UNKNOWN_TOKEN'\n",
    "VOCAB_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading comments! (wait for minutes...)\n",
      "Split Done\n",
      "Finished:  0.653252\n"
     ]
    }
   ],
   "source": [
    "t = datetime.now()\n",
    "with open('/home/anderson/Dropbox/reddit_comments_small.csv', 'rt') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    \n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(\n",
    "        *[nltk.sent_tokenize(x[0].decode('UTF-8').lower()) for x in reader])\n",
    "    print 'Split Done'\n",
    "    \n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (SENTENCE_START, x, SENTENCE_END)\n",
    "                    for x in sentences ]\n",
    "    print 'Finished: ', (datetime.now() - t).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(st) for st in sentences ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17197 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"%d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Indexing\n",
    "\n",
    "아래 2개의 함수를 사용해서 word -> index 또는 그 반대로 변환을 해 줄수 있습니다.\n",
    "\n",
    "* index_to_word(index) : list -> word\n",
    "* word_to_index(word) : dict -> integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 8000\n",
      "Unknown token Index: 7999\n",
      "The least frequent word in our vocabulary is '//imgur.com/idqgygh' and appeared 1 times.\n"
     ]
    }
   ],
   "source": [
    "most_common_freq = word_freq.most_common(VOCAB_SIZE-1)\n",
    "index_to_word = [x[0] for x in most_common_freq]\n",
    "index_to_word.append(UNKNOWN_TOKEN)\n",
    "word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "\n",
    "print \"VOCAB_SIZE: %d\" % VOCAB_SIZE\n",
    "print \"Unknown token Index:\", word_to_index[UNKNOWN_TOKEN]\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % \\\n",
    "          (most_common_freq[-1][0], most_common_freq[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Unknown Tokens\n",
    "\n",
    "링크, 이상한 단어들.. 등등. 모든 단어들을 모두 외우고 있을수는 없습니다.<br>\n",
    "따라서 가장 많이 나온 단어들 (word_freq.most_common(VOCAB_SIZE-1)) 을 제외하고, 그 외 단어들은 UNKNOWN_TOKEN으로 대체합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 68, 522, 67, 35, 91, 20, 7999, 32, 12, 3026, 34]\n",
      "[68, 522, 67, 35, 91, 20, 7999, 32, 12, 3026, 34, 1]\n"
     ]
    }
   ],
   "source": [
    "# Create the training data\n",
    "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "print x_train[1]\n",
    "print y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(rnn)\n",
    "model = rnn.RNNNumpy(VOCAB_SIZE)\n",
    "o, s = model.forward_propagation(x_train[9])\n",
    "p = model.predict(x_train[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error cost: 8.98747007115\n"
     ]
    }
   ],
   "source": [
    "print 'error cost:', model.cross_entropy(x_train[26], y_train[26])\n",
    "model.bptt(x_train[26], y_train[26])\n",
    "model.calculate_gradients(x_train[26], y_train[26])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Total Data:  11587\n",
      "2016-09-14 21:11:37.062952: i=0, cost=8.98722103854\n",
      "2016-09-14 21:11:44.719646: i=100, cost=8.98706676536\n",
      "2016-09-14 21:11:52.438135: i=200, cost=8.98683016155\n",
      "2016-09-14 21:12:00.514727: i=300, cost=8.98473871271\n",
      "2016-09-14 21:12:08.149691: i=400, cost=8.98729998738\n",
      "2016-09-14 21:12:15.803158: i=500, cost=8.97605857328\n",
      "2016-09-14 21:12:22.979648: i=600, cost=8.97772101904\n",
      "2016-09-14 21:12:30.504268: i=700, cost=8.97798193959\n",
      "2016-09-14 21:12:37.881149: i=800, cost=8.9614174036\n",
      "2016-09-14 21:12:45.391671: i=900, cost=8.94699520533\n",
      "2016-09-14 21:12:53.085263: i=1000, cost=8.83650153805\n",
      "2016-09-14 21:13:00.471641: i=1100, cost=8.51452153067\n",
      "2016-09-14 21:13:07.530092: i=1200, cost=8.85190189799\n",
      "2016-09-14 21:13:15.423381: i=1300, cost=8.86469610436\n",
      "2016-09-14 21:13:22.147843: i=1400, cost=6.01450408332\n",
      "2016-09-14 21:13:30.704227: i=1500, cost=6.74473788643\n",
      "2016-09-14 21:13:39.215563: i=1600, cost=8.63738353483\n",
      "2016-09-14 21:13:46.629712: i=1700, cost=9.59624283949\n",
      "2016-09-14 21:13:54.650601: i=1800, cost=8.42457231861\n",
      "2016-09-14 21:14:01.593971: i=1900, cost=8.21519673404\n",
      "2016-09-14 21:14:09.668965: i=2000, cost=8.20603334918\n",
      "2016-09-14 21:14:17.227251: i=2100, cost=8.73321170993\n",
      "2016-09-14 21:14:25.235726: i=2200, cost=9.49566772842\n",
      "2016-09-14 21:14:36.129718: i=2300, cost=9.15625009088\n",
      "2016-09-14 21:14:44.806303: i=2400, cost=8.78825085578\n",
      "2016-09-14 21:14:54.174840: i=2500, cost=8.32142996433\n",
      "2016-09-14 21:15:02.737026: i=2600, cost=8.12649386992\n",
      "2016-09-14 21:15:10.280867: i=2700, cost=8.8277176502\n",
      "2016-09-14 21:15:17.849104: i=2800, cost=7.79129967738\n",
      "2016-09-14 21:15:26.192056: i=2900, cost=7.64976403866\n",
      "2016-09-14 21:15:33.876008: i=3000, cost=6.71778456044\n",
      "2016-09-14 21:15:41.906282: i=3100, cost=7.63443887941\n",
      "2016-09-14 21:15:57.004164: i=3200, cost=6.90501920614\n",
      "2016-09-14 21:16:07.455057: i=3300, cost=6.51081112251\n",
      "2016-09-14 21:16:16.422934: i=3400, cost=6.560442737\n",
      "2016-09-14 21:16:24.810238: i=3500, cost=4.97281126391\n",
      "2016-09-14 21:16:32.431877: i=3600, cost=5.83772322047\n",
      "2016-09-14 21:16:40.554718: i=3700, cost=6.03692333279\n",
      "2016-09-14 21:16:48.154926: i=3800, cost=5.44036291993\n",
      "2016-09-14 21:16:55.916507: i=3900, cost=6.28356641709\n",
      "2016-09-14 21:17:03.194054: i=4000, cost=6.0658727059\n",
      "2016-09-14 21:17:10.844946: i=4100, cost=5.69005061389\n",
      "2016-09-14 21:17:18.100402: i=4200, cost=7.27135678472\n",
      "2016-09-14 21:17:26.017932: i=4300, cost=6.63787223099\n",
      "2016-09-14 21:17:32.848825: i=4400, cost=6.1262239293\n",
      "2016-09-14 21:17:40.260044: i=4500, cost=6.31670188593\n",
      "2016-09-14 21:17:48.481994: i=4600, cost=6.34912176827\n",
      "2016-09-14 21:17:56.110022: i=4700, cost=6.5036534478\n",
      "2016-09-14 21:18:03.480277: i=4800, cost=5.9232564405\n",
      "2016-09-14 21:18:10.687237: i=4900, cost=6.3891409132\n",
      "2016-09-14 21:18:18.146060: i=5000, cost=6.17172466946\n",
      "2016-09-14 21:18:25.087587: i=5100, cost=5.17243532566\n",
      "2016-09-14 21:18:32.346180: i=5200, cost=6.21993582717\n",
      "2016-09-14 21:18:39.470156: i=5300, cost=5.16604764127\n",
      "2016-09-14 21:18:46.458675: i=5400, cost=6.50297459818\n",
      "2016-09-14 21:18:53.535981: i=5500, cost=7.34898797724\n",
      "2016-09-14 21:19:00.162263: i=5600, cost=5.97516912458\n",
      "2016-09-14 21:19:06.872151: i=5700, cost=5.96160151709\n",
      "2016-09-14 21:19:13.752848: i=5800, cost=5.54379368625\n",
      "2016-09-14 21:19:20.780625: i=5900, cost=5.66010850409\n",
      "2016-09-14 21:19:28.047045: i=6000, cost=6.14277950815\n",
      "2016-09-14 21:19:35.005467: i=6100, cost=6.01726320261\n",
      "2016-09-14 21:19:42.097826: i=6200, cost=6.33720565707\n",
      "2016-09-14 21:19:49.500385: i=6300, cost=4.08834697927\n",
      "2016-09-14 21:19:56.181856: i=6400, cost=6.59577746471\n",
      "2016-09-14 21:20:03.279381: i=6500, cost=5.96720249664\n",
      "2016-09-14 21:20:11.585352: i=6600, cost=5.96685949477\n",
      "2016-09-14 21:20:18.521394: i=6700, cost=5.92982350266\n",
      "2016-09-14 21:20:26.604907: i=6800, cost=5.60332613268\n",
      "2016-09-14 21:20:33.646425: i=6900, cost=4.94057301328\n",
      "2016-09-14 21:20:41.051137: i=7000, cost=4.95602752699\n",
      "2016-09-14 21:20:48.488345: i=7100, cost=5.57347638803\n",
      "2016-09-14 21:20:55.528370: i=7200, cost=5.0870427287\n",
      "2016-09-14 21:21:03.513205: i=7300, cost=5.42790933007\n",
      "2016-09-14 21:21:11.396928: i=7400, cost=5.80237221087\n",
      "2016-09-14 21:21:18.476213: i=7500, cost=2.64596802476\n",
      "2016-09-14 21:21:26.037423: i=7600, cost=5.53230756829\n",
      "2016-09-14 21:21:32.767331: i=7700, cost=5.87076217528\n",
      "2016-09-14 21:21:39.759561: i=7800, cost=4.18933174545\n",
      "2016-09-14 21:21:46.216957: i=7900, cost=6.21351753119\n",
      "2016-09-14 21:21:53.300442: i=8000, cost=6.1646125666\n",
      "2016-09-14 21:22:00.619438: i=8100, cost=4.95446988326\n",
      "2016-09-14 21:22:08.137921: i=8200, cost=6.25950370266\n",
      "2016-09-14 21:22:15.586968: i=8300, cost=6.60352656247\n",
      "2016-09-14 21:22:23.259929: i=8400, cost=5.99704349315\n",
      "2016-09-14 21:22:31.255367: i=8500, cost=5.77776122941\n",
      "2016-09-14 21:22:38.549939: i=8600, cost=3.45186574705\n",
      "2016-09-14 21:22:46.426696: i=8700, cost=6.62499644362\n",
      "2016-09-14 21:22:54.412385: i=8800, cost=6.37622350773\n",
      "2016-09-14 21:23:01.362672: i=8900, cost=5.7979016308\n",
      "2016-09-14 21:23:08.568405: i=9000, cost=6.41677337522\n",
      "2016-09-14 21:23:16.370220: i=9100, cost=6.99172915876\n",
      "2016-09-14 21:23:24.376926: i=9200, cost=6.38878477049\n",
      "2016-09-14 21:23:31.112356: i=9300, cost=6.59227900237\n",
      "2016-09-14 21:23:39.140548: i=9400, cost=5.78354670473\n",
      "2016-09-14 21:23:46.603803: i=9500, cost=5.08858775444\n",
      "2016-09-14 21:23:53.770159: i=9600, cost=6.65191983804\n",
      "2016-09-14 21:24:02.232350: i=9700, cost=4.84832205213\n",
      "2016-09-14 21:24:09.586792: i=9800, cost=5.21658302324\n",
      "2016-09-14 21:24:16.657397: i=9900, cost=6.05153659365\n",
      "2016-09-14 21:24:24.295911: i=10000, cost=5.8494059312\n",
      "2016-09-14 21:24:31.401925: i=10100, cost=7.21231350995\n",
      "2016-09-14 21:24:38.660163: i=10200, cost=4.77916238835\n",
      "2016-09-14 21:24:45.552506: i=10300, cost=6.56624655337\n",
      "2016-09-14 21:24:52.523855: i=10400, cost=5.66066433739\n",
      "2016-09-14 21:25:00.700387: i=10500, cost=4.91175574618\n",
      "2016-09-14 21:25:08.437268: i=10600, cost=6.20711969239\n",
      "2016-09-14 21:25:15.990122: i=10700, cost=6.28930423632\n",
      "2016-09-14 21:25:24.339468: i=10800, cost=6.57719860255\n",
      "2016-09-14 21:25:32.058649: i=10900, cost=5.94033968596\n",
      "2016-09-14 21:25:40.276340: i=11000, cost=6.23836785451\n",
      "2016-09-14 21:25:51.459764: i=11100, cost=6.56531590921\n",
      "2016-09-14 21:25:59.331125: i=11200, cost=6.10941330555\n",
      "2016-09-14 21:26:06.943918: i=11300, cost=5.40389407523\n",
      "2016-09-14 21:26:13.945487: i=11400, cost=6.20322773489\n",
      "2016-09-14 21:26:22.065005: i=11500, cost=5.57731626053\n"
     ]
    }
   ],
   "source": [
    "model.train(x_train, y_train, npoch=len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_idx_to_sentence(index):\n",
    "    return ' '.join([index_to_word[i] for i in index])\n",
    "\n",
    "def convert_sentence_to_idx(sentence):\n",
    "    return  [ word_to_index[w] for w in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result1: [   7    3 7999    3    3 7999]\n",
      "_SETENCE_START in ny everything is so\n",
      "i the _UNKNOWN_TOKEN the the _UNKNOWN_TOKEN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test1 = convert_sentence_to_idx(SENTENCE_START+ \" in ny everything is so\")\n",
    "result1 = model.predict(test1)\n",
    "print 'result1:', result1\n",
    "print convert_idx_to_sentence(test1)\n",
    "print convert_idx_to_sentence(result1)\n",
    "\n",
    "print "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
