{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# AdaGrad\n",
    "\n",
    "* [Deep Learning 책 참고 299page](https://books.google.co.kr/books/about/Deep_Learning.html?id=Np9SDQAAQBAJ&redir_esc=y)\n",
    "* [An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/index.html#fn:24)\n",
    "\n",
    "Stochastic Gradient Descent (SGD)는 convex 또는 non-convex function의 optimization에 일반적으로 사용되는 알고리즘입니다.<br>\n",
    "SGD의 한가지 문제점은 learning rate에 매우 민감합니다. 특히 데이터가 sparse이며 features가 서로다른 frequencies (빈도수)를 갖고 있다면, 단 하나의 learning rate가 모든 weight update에 영향을 주는것은 문제가 될 수 있습니다. \n",
    "\n",
    "이전의 Momentum update에서는 다음과 같은 공식을 사용했습니다. \n",
    "\n",
    "$$ \\begin{align}\n",
    "v &= \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta; x^{(i)},y^{(i)}) \\\\\n",
    "\\theta &= \\theta - v\n",
    "\\end{align} $$\n",
    "\n",
    "즉 모든 weights $ \\theta $에 대해서 동일한 learning rate $ \\eta $ 를 사용하여 update를 하였습니다. \n",
    "\n",
    "\n",
    "AgaGrad는 각각의 데이터에 dynamically adapt함으로서 궁극적으로 각각의 feature들마다 서로다른 learning rate로 연산을 합니다.<br>\n",
    "쉽게 이야기해서, 빈도수가 낮은 데이터에는 높은 learning rate를 적용하고, 빈도수가 높은 데이터에는 낮은 learning rate를 자동으로 적용합니다. <br>\n",
    "따라서 sparse한 데이터에 적합하며, 대표적인 예가 구글 X lab에서 16000대의 cores를 사용하여 유튜브 동영상안의 고양이를 detection하는 모델에 Adagrad를 사용한것입니다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Global Learning Rate $ \\eta $ 가 필요합니다. \n",
    "* weights $ \\theta $에 대한 초기화가 필요합니다.\n",
    "* small constant $ \\epsilon $ 값을 설정합니다. (numerical stability를 위해서 약 $ 10^{-8} $로 설정, 즉 분모에 들어가게 되는데 0이 안되도록 매우 작은 수를 넣음)\n",
    "* gradient accumulation variable $ G $값은 weight와 동일한 shape인 0으로 초기화합니다.\n",
    "\n",
    "먼저 gradient를 구한뒤 sqaured gradient (gradient의 제곱)을 r에 accumulation해줍니다.\n",
    "\n",
    "$$ \\begin{align}\n",
    "g_t &= \\nabla_{\\theta} J(\\theta; x^{(i)}, y^{(i)}) \\\\ \n",
    "G_t &= G_t + g_t \\odot g_t \n",
    "\\end{align} $$ \n",
    "\n",
    "update를 계산합니다. \n",
    "\n",
    "$$ \\Delta \\theta_t =  \\frac{\\eta}{\\epsilon + \\sqrt{G_t}} \\odot g_t $$\n",
    "\n",
    "update를 적용합니다. \n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t - \\Delta \\theta_t $$\n",
    "\n",
    "> $ \\odot $ 기호는 element wise multiplication 입니다 \n",
    "\n",
    "AdaGrad의 장점은 보통 다른 optimizers의 경우에는 learning rate를 일반적으로 0.01로 놓고 변경되지 않지만, Adagram같은 경우는 자동으로 learning rate를 계산해준다는 것입니다. 반면 약점은 denominator부분의 accumulation of the squared gradients의 값이 지속적으로 쌓여서 계속 늘어난다는 단점이 있습니다. 결국 learning rate는 점점 작아진다는 약점을 갖고있습니다. 이러한 약점은 극복한 것이 Adadelta입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0d5ddbf5c0>"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npqQhIiKh/Q+fgQCyBxRqPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d5e031e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "# setosa_x = iris.data[:50]\n",
    "# setosa_y = iris.target[:50]\n",
    "# versicolor_x = iris.data[50:100]\n",
    "# versicolor_y = iris.target[50:100]\n",
    "# scatter(setosa_x[:, 0], setosa_x[:, 2])\n",
    "# scatter(versicolor_x[:, 0], versicolor_x[:, 2])\n",
    "\n",
    "# Extract sepal length, petal length from Setosa and Versicolor\n",
    "data = iris.data[:100, [0, 2]]\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "# Split data to test and train data\n",
    "train_x, test_x, train_y, test_y = train_test_split(data, iris.target[:100], test_size=0.3)\n",
    "\n",
    "# Plotting data\n",
    "scatter(data[:50, 0], data[:50, 1], label='Setosa')\n",
    "scatter(data[50:100, 0], data[50:100, 1], label='Versicolour')\n",
    "title('Iris Data')\n",
    "xlabel('sepal length')\n",
    "ylabel('petal length')\n",
    "grid()\n",
    "legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Stochastic Gradient Descent with AdaGrad Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0 \t\tweights: [ 0.13311942 -0.13196347 -0.70207509]\n",
      "Accuracy: 0.0 \t\tweights: [ 0.17517404 -0.02139136 -0.5622791 ]\n",
      "Accuracy: 0.03 \t\tweights: [ 0.22024669  0.08566276 -0.42306429]\n",
      "Accuracy: 0.4 \t\tweights: [ 0.26857055  0.1866223  -0.28535928]\n",
      "Accuracy: 0.6 \t\tweights: [ 0.31970458  0.27497968 -0.15187859]\n",
      "Accuracy: 0.83 \t\tweights: [ 0.37118095  0.33185828 -0.03023366]\n",
      "Accuracy: 0.9 \t\tweights: [ 0.41775365  0.33561797  0.06756387]\n",
      "Accuracy: 0.93 \t\tweights: [ 0.45451793  0.30855672  0.14049942]\n",
      "Accuracy: 0.97 \t\tweights: [ 0.48013352  0.27088363  0.19729631]\n",
      "Accuracy: 0.97 \t\tweights: [ 0.49592062  0.23050147  0.24497723]\n",
      "Accuracy: 0.97 \t\tweights: [ 0.50425995  0.19049694  0.28725931]\n",
      "Accuracy: 0.97 \t\tweights: [ 0.50761714  0.15210897  0.32572912]\n",
      "Accuracy: 0.97 \t\tweights: [ 0.50804359  0.11597703  0.36093549]\n",
      "Accuracy: 0.97 \t\tweights: [ 0.50702581  0.08260442  0.39296609]\n",
      "Accuracy: 1.0 \t\tweights: [ 0.50552161  0.05245024  0.42172069]\n",
      "Accuracy: 1.0 \t\tweights: [ 0.50406479  0.02586577  0.44707643]\n",
      "Accuracy: 1.0 \t\tweights: [ 0.50288567  0.00299816  0.46900527]\n",
      "Accuracy: 1.0 \t\tweights: [ 0.50202729 -0.01625318  0.48763113]\n",
      "Accuracy: 1.0 \t\tweights: [ 0.50144198 -0.03219678  0.50321966]\n",
      "Accuracy: 1.0 \t\tweights: [ 0.50105654 -0.0452585   0.51612571]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([ 0.09370901, -0.24480254, -0.84210235]) # np.random.randn(2 + 1)\n",
    "\n",
    "def predict(w, x):\n",
    "    N = len(x)\n",
    "    yhat = w[1:].dot(x.T) + w[0]\n",
    "    return yhat\n",
    "\n",
    "def  adagrad_nn2(w, X, Y, eta=0.01, acmu_size=1):\n",
    "    \"\"\"\n",
    "    @param eta <float>: learning rate\n",
    "    @param acmu_size <int>: the size of the accumulation of the squred of the gradients with regard to theta\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    e = 1e-8\n",
    "    G = np.zeros_like(w)\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(N):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        yhat = predict(w, x)\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        gradient_w = 2/N*-(y-yhat)*x\n",
    "        gradient_b = 2/N*-(y-yhat)\n",
    "        \n",
    "        G[1:] += gradient_w * gradient_w\n",
    "        G[0] += gradient_b * gradient_b\n",
    "        delta_w = eta/(e + np.sqrt(G[1:])) * gradient_w\n",
    "        delta_b = eta/(e + np.sqrt(G[0])) * gradient_b\n",
    "        w[1:] = w[1:] - delta_w\n",
    "        w[0] = w[0] - delta_b\n",
    "        \n",
    "        if np.linalg.norm(w) < 1e-3:\n",
    "            break\n",
    "    return w\n",
    "\n",
    "for i in range(20):\n",
    "    w = adagrad_nn2(w, train_x, train_y)\n",
    "    yhats = predict(w, test_x)\n",
    "    yhats = np.where(yhats >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(test_y, yhats)\n",
    "    print('Accuracy:', round(accuracy, 2), '\\t\\tweights:', w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
