{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# AdaDelta\n",
    "\n",
    " * [ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Abstract\n",
    "\n",
    "AdaDelta는 per-dimension learning rate method를 갖고있으며,\n",
    "첫번째     Vanilla stochastic gradient descent보다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradient Descent 의 문제.. \n",
    "\n",
    "많은 머신러닝 알고리즘들은 a set of parameters $ \\theta $ 값을 objective function $ f(\\theta) $를 optimize함으로서 update해줍니다.<br>\n",
    "이러한 알고리즘들은 보통 iterative procedure를 갖고 있으며, 매 iteration마다  change to the parameters인 $ \\Delta \\theta $를 적용시킵니다. \n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t + \\Delta \\theta_t $$\n",
    "\n",
    "이때 negative gradient $ g_t $를 사용합니다. 즉..\n",
    "\n",
    "$$ \\Delta \\theta_t = - \\eta g_t $$\n",
    "\n",
    "여기서 $ g_t $는 partial derivative이고, $ \\eta $는 learning rate를 가르킵니다.\n",
    "\n",
    "$$ g_t = \\frac{\\partial f(\\theta_t)}{\\partial \\theta_t} $$\n",
    "\n",
    "보통 SGD 또는 Mini-batch GD를 사용하게 되는데.. 문제는 learning rate를 명시해야 하며, learning rate에 따라서 결과값이 크게 영향받을수 있습니다.<br>\n",
    "즉 learning rate를 크게 잡으면 diverge 되거나 낮게 잡으면 learning이 매우 느려지게 됩니다. <br>\n",
    "따라서 Model에 대한 learning rate를 제대로 잡아야 하는데... 문제는 이게 사람이 잡는다는것이 문제. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Per-dimension First Order Methods \n",
    "\n",
    "### Momentum\n",
    "\n",
    "가장 중요한 핵심은 gradient $ g_t $ 지속적으로 같은 방향을 가르키며, gradient의 부호(sign...- 또는 +)가 지속적으로 변한다면.. progress를 slow시키는 것입니다. <br>\n",
    "이는 past parameter update $ \\Delta \\theta_{t-1} $, 그리고 exponential decay $ \\gamma $를 사용함으로서 가능해집니다.\n",
    "\n",
    "$$ \\Delta \\theta_t = \\gamma \\Delta \\theta_{t-1} - \\eta g_t $$\n",
    "\n",
    "여기서 $ \\gamma $는 constant값으로서 이전 parameter update $ \\Delta \\theta_{t-1} $을 control 합니다.<br>\n",
    "\n",
    "SGD에서 progress는 매우 느립니다. 왜냐하면 gradient magnitude은 매우 작으며, <br>\n",
    "fixed global learning rate (게다가 모든 dimensions에 적용) progress를 더더욱 느리게 만들어버립니다.\n",
    "\n",
    "높은 learning rate를 사용하면 valley를 서로 왔다갔다하는 oscillations을 만들수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "update rule은 다음과 같습니다. \n",
    "\n",
    "$$ \\Delta \\theta_t = - \\frac{\\eta}{\\sqrt{ \\sum^t_{τ=1} g^2_τ }} $$\n",
    "\n",
    "여기서 denominator의 경우 l2 norm 의 모든 이전의 gradients를 연산합니다. <br>\n",
    "$ \\eta $의 경우 global learning rate입니다.\n",
    "\n",
    "물론 global learning rate가 있지만, 각각의 $ \\theta $의 dimension마다 각각의 dynamic rate를 갖고 있습니다. <br>\n",
    "dynamic rate는 gradient magnitude와 반대로 커나가기 때문에, large gradient의 경우 작은 learning rate갖으며, small gradient의 경우에는 large learning rate를 갖습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Order Methods\n",
    "\n",
    "### AdaDelta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
