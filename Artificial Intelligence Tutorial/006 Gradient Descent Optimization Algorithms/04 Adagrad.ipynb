{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad\n",
    "\n",
    "Stochastic Gradient Descent (SGD)는 convex 또는 non-convex function의 optimization에 일반적으로 사용되는 알고리즘입니다.<br>\n",
    "SGD의 한가지 문제점은 learning rate에 매우 민감합니다. 특히 데이터가 sparse이며 features가 서로다른 frequencies (빈도수)를 갖고 있다면, 단 하나의 learning rate가 모든 weight update에 영향을 주는것은 문제가 될 수 있습니다. \n",
    "\n",
    "AgaGrad는 각각의 데이터에 dynamically adapt함으로서 궁극적으로 각각의 feature들마다 서로다른 learning rate로 연산을 합니다.<br>\n",
    "쉽게 이야기해서, 빈도수가 낮은 데이터에는 높은 learning rate를 적용하고, 빈도수가 높은 데이터에는 낮은 learning rate를 자동으로 적용합니다. <br>\n",
    "따라서 sparse한 데이터에 적합하며, 대표적인 예가 구글 X lab에서 16000대의 cores를 사용하여 유튜브 동영상안의 고양이를 detection하는 모델에 Adagrad를 사용한것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 Momentum update에서는 다음과 같은 공식을 사용했습니다. \n",
    "\n",
    "$$ \\begin{align}\n",
    "v &= \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta; x^{(i)},y^{(i)}) \\\\\n",
    "\\theta &= \\theta - v\n",
    "\\end{align} $$\n",
    "\n",
    "즉 모든 weights $ \\theta $에 대해서 동일한 learning rate $ \\eta $ 를 사용하여 update를 하였습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaGrad**에서는 매 다른 step $ t $마다, 각각의 weight $ \\theta_i $ 에 대해서, 각각 다른 learning rate를 사용하여 update를 합니다. <br>\n",
    "먼저 gradient objective function $ g_{t, i} $을 다음과 같이 정의 합니다. \n",
    "\n",
    "$$ g_{t, i} = \\nabla_\\theta J( \\theta_i ) $$\n",
    "\n",
    "각 time step $ t $마다, weights에 대한 업데이트는 다음과 같이 합니다. \n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\eta \\cdot g_{t, i} $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad Model\n",
    "\n",
    "[Deep Learning 책 참고 299page](https://books.google.co.kr/books/about/Deep_Learning.html?id=Np9SDQAAQBAJ&redir_esc=y)\n",
    "\n",
    "* Global Learning Rate $ \\eta $ 가 필요합니다. \n",
    "* weights $ \\theta $에 대한 초기화가 필요합니다.\n",
    "* small constant $ \\epsilon $ 값을 설정합니다. (numerical stability를 위해서 약 $ 10^{-8} $로 설정, 즉 분모에 들어가게 되는데 0이 안되도록 매우 작은 수를 넣음)\n",
    "* gradient accumulation variable $ r = 0 $ 초기화합니다.\n",
    "\n",
    "먼저 gradient를 구한뒤 sqaured gradient (gradient의 제곱)을 r에 accumulation해줍니다.\n",
    "\n",
    "$$ \\begin{align}\n",
    "g &= \\nabla_{\\theta} J(\\theta; x^{(i)}, y^{(i)}) \\\\ \n",
    "r &= r + g \\circledcirc g \n",
    "\\end{align} $$ \n",
    "\n",
    "update를 계산합니다. \n",
    "\n",
    "$$ \\Delta \\theta = - \\frac{\\eta}{\\epsilon + \\sqrt{r}} \\circledcirc g $$\n",
    "\n",
    "update를 적용합니다. \n",
    "\n",
    "$$ \\theta = \\theta + \\Delta \\theta $$\n",
    "\n",
    "> $ \\circledcirc $ 기호는 element wise multiplication 입니다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "* [Credit Card Fraud Detect - Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud)에서 다운로드 받을수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (213605, 29)\n",
      "Train Y shape: (213605,)\n",
      "Test X shape: (71202, 29)\n",
      "Test Y shape: (71202,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/credit-card-fraud-detection/creditcard.csv')\n",
    "\n",
    "# Preprocessing Amount\n",
    "amt_scale = StandardScaler()\n",
    "data['NormAmount'] =  amt_scale.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Split Train and Test Data\n",
    "X = data.drop(['Time', 'Amount', 'Class'], axis=1).as_matrix()\n",
    "Y = data['Class'].as_matrix()\n",
    "# Y = Y.reshape(Y.shape[0], 1)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.25, random_state=1)\n",
    "print('Train X shape:', train_x.shape)\n",
    "print('Train Y shape:', train_y.shape)\n",
    "print('Test X shape:', test_x.shape)\n",
    "print('Test Y shape:', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent with AdaGrad Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45906013876\n"
     ]
    }
   ],
   "source": [
    "w = np.random.randn(29 + 1)\n",
    "\n",
    "def predict(w, x):\n",
    "    N = len(x)\n",
    "    yhat = w[1:].dot(x.T) + w[0]\n",
    "    return yhat\n",
    "\n",
    "def adagrad_nn(w, X, Y, eta=0.1, ):\n",
    "    N = len(X)\n",
    "    e = 1e-8\n",
    "    r = 0\n",
    "    for i in range(N):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        yhat = predict(w, x)\n",
    "        gradient = np.sum(-(y - yhat) * x)\n",
    "        r = r + gradient * gradient\n",
    "        \n",
    "        update = -eta/(e + np.sqrt(r)) * gradient\n",
    "        w = w + update\n",
    "        \n",
    "    return w\n",
    "    \n",
    "\n",
    "for i in range(1):\n",
    "    w = adagrad_nn(w, train_x, train_y)\n",
    "    yhats = predict(w, test_x)\n",
    "    yhats = np.where(yhats >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(test_y, yhats)\n",
    "    print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
