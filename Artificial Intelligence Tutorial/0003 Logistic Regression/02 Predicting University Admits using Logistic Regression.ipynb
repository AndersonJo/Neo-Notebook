{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Predicting University Admits using Logistic Regression\n",
    "\n",
    "Logistic Regression에서 weights값들을 학습시키는 방법을 기술합니다. \n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Sum-squred error 에서 cost값을 낮추려고 하였습니다. <br>\n",
    "Logistic Regression에서 사용되는 cost function을 알기 위해서는 Likelihood를 먼저 알아야 하고, <br>\n",
    "Likelihood는 sum-squred error와는 다르게 maximize해야 합니다. \n",
    "\n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\" style=\"color:#777;\"> 여기서 bold체는 series (list)입니다.</i>\n",
    "\n",
    "### $$ L(\\mathbf{w}) = P(\\mathbf{y}\\ |\\ \\mathbf{x};\\mathbf{w}) = \\prod^n_{i=1} P(y^{(i)}\\ |\\ x^{(i)}; \\mathbf{w}) = \\prod^n_{i=1} \\left( \\phi(z^{(i)} \\right)^{y^{(i)}} \\left(1 - \\phi(z^{(i)}) \\right)^{1-y^{(i)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood function\n",
    "\n",
    "실제로는 likelihood가 그대로 쓰이는 경우는 없고, 계산의 편의상 log-likelihood를 사용하는 경우가 일반적입니다.\n",
    "\n",
    "* log (natural)를 붙임으로서 numerical underflow (likelihood값이 너무 작을때 발생) 를 낮춰줍니다.\n",
    "* product of factors 를 summation of factors로 바꿔줌으로서, derivative를 구할때 좀더 쉽게 계산할수 있도록 해줍니다.\n",
    "\n",
    "\n",
    "### $$ l(\\mathbf{w}) = \\ln L(\\mathbf{w}) = \\sum^n_{i=1} y^{(i)}  \\ln \\left( \\phi(z^{(i)}) \\right) + \\left(1 - y^{(i)} \\right) \\ln \\left( 1 - \\phi(z^{(i)}) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "Grandient ascent를 사용해서 위의 log-likelihood를 그대로 사용할수도 있습니다.<br>\n",
    "하지만 log-likelihood를 약간 변형하여 cost function으로 만들어주고 여기에 gradient descent를 사용할수 있도록 하겠습니다.<br>\n",
    "그냥 위의 공식에서 minus를 2개를 더 붙여준것밖에 달라진것이 없습니다.\n",
    "\n",
    "### $$ J(\\mathbf{w}) = \\sum^n_{i=0} -y^{(i)} \\ln \\left( \\phi(z^{(i)}) \\right) - \\left(1 - y^{(i)} \\right) \\ln \\left( 1 - \\phi(z^{(i)}) \\right)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent는 2가지 방법으로 process될 수 있습니다. <br>\n",
    "하나는 log-likelihood를 maximize하는 방법과, 다른 하나는 위의 cost function을 값을 minimize 하는 방법입니다.<br>\n",
    "먼저 log-likelihood를 maximize하려면 partial derivative of the log-likelihood function을 알아야 합니다.\n",
    "\n",
    "아래는 $ w_j $ weight의 partial derivative를 구하는 것입니다.<br>\n",
    "즉 전체 weights값이 아니기 때문에 $ \\sum $ 같은것이 빠져있습니다.\n",
    "\n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\" style=\"color:#777;\"> 아래의 공식은 실제 집접 구한 공식이기 때문에 틀린부분이 있을수도 있습니다.</i>\n",
    "\n",
    "<span class=\"fa fa-exclamation-circle\" aria-hidden=\"true\" style=\"color:#777;\"></span> $ \\frac{d}{dx} \\ln x = \\frac{1}{x}$\n",
    "\n",
    "<span class=\"fa fa-exclamation-circle\" aria-hidden=\"true\" style=\"color:#777;\"></span> $ \\frac{d}{dx} \\ln f(x) = \\frac{1}{f(x)} f^{\\prime}(x)$\n",
    "\n",
    "### $$ \\frac{\\partial}{\\partial w_j} l(\\mathbf{w}) = y \\cdot \\ln(\\phi(z)) + (1-y) \\cdot \\ln(1-\\phi(z)) $$\n",
    "\n",
    "### $$ = \\frac{\\partial}{\\partial w_j} y \\cdot \\ln(\\phi(z)) + \\frac{\\partial}{\\partial w_j} (1-y) \\cdot \\ln(1-\\phi(z))  $$\n",
    "\n",
    "### $$ = y \\cdot \\frac{1}{\\phi(z)} \\cdot \\frac{\\partial}{\\partial w_j} \\phi(z) + (1-y) \\cdot \\frac{1}{1-\\phi(z)} \\cdot \\frac{\\partial}{\\partial w_j} (1 - \\phi(z)) $$\n",
    "\n",
    "### $$ = \\left( y \\cdot \\frac{1}{\\phi(z)} - (1-y) \\cdot \\frac{1}{1-\\phi(z)} \\right) \\frac{\\partial}{\\partial w_j} \\phi(z) $$\n",
    "\n",
    "여기에서 derivative of the sigmoid function 값인 $ \\phi(z)(1-\\phi(z)) $를 대입해줍니다.\n",
    "\n",
    "<span class=\"fa fa-exclamation-circle\" aria-hidden=\"true\" style=\"color:#777;\"></span> $ \\frac{d}{dx} f(g(x)) = f^{\\prime}(g(x)) \\cdot g^{\\prime}(x)$\n",
    "\n",
    "<span class=\"fa fa-exclamation-circle\" aria-hidden=\"true\" style=\"color:#777;\"></span> $ \\frac{d}{dx} z = w_j x_j = 1 \\cdot x_j $\n",
    "\n",
    "### $$ =  \\left( y \\cdot \\frac{1}{\\phi(z)} - (1-y) \\cdot \\frac{1}{1-\\phi(z)} \\right) \\phi(z)(1-\\phi(z)) \\frac{\\partial}{\\partial w_j} z  $$\n",
    "\n",
    "### $$ = \\left( y \\cdot (1-\\phi(z)) - (1-y) \\cdot \\phi(z) \\right) x_j $$\n",
    "\n",
    "### $$ = ((y - \\phi(z)) \\cdot x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목표를 다시 상기시키면, maximize the log-likelihood 입니다. <br>\n",
    "즉 포인트는 negative gradient ($ \\Delta w = -\\eta \\nabla l(\\mathbf{w}) $)를 하는게 아니라 likelihood를 최대한 키우는 것 입니다.<br>\n",
    "(즉 앞에 minus기호를 뺍니다.)\n",
    "\n",
    "### $$ w_j := w_j + \\eta \\sum^n_{i=1} \\left( y^{i} - \\phi(z^{(i)})  \\right) x_j $$\n",
    "\n",
    "위 공식의 일반적인 표현은 다음과 같이 합니다.\n",
    "\n",
    "### $$ \\mathbf{w} := \\mathbf{w} +  \\Delta \\mathbf{w} $$\n",
    "\n",
    "### $$ \\Delta \\mathbf{w} = \\eta \\nabla l(\\mathbf{w})$$\n",
    "\n",
    "반대로 cost function $ J $의 값을 낮춰서 Gradient descent를 구현할수도 있습니다.\n",
    "\n",
    "### $$ \\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j} = \\eta \\sum^n_{i=1} \\left( y^{(i)} - \\phi(z^{(i)}) \\right) x^{(i)}_j $$\n",
    "\n",
    "### $$ \\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w}, \\Delta \\mathbf{w} = - \\eta \\nabla J(\\mathbf{w}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('../../data/basic_csv_data/binary.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "# Shuffle\n",
    "data = data[np.random.permutation(len(data))]\n",
    "\n",
    "# Seperate X and Y\n",
    "Y = data[:, 0] # 0:불합격, 1:합격 \n",
    "X = data[:, 1:] # GRE, GPA, Rank\n",
    "\n",
    "# STD Standardization \n",
    "# X[:, 0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\n",
    "# X[:, 1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n",
    "# X[:, 2] = (X[:,2] - X[:,2].mean()) / X[:,2].std()\n",
    "\n",
    "# Min-max Standardization\n",
    "X[:, 0] = (X[:, 0] - X[:,0].min()) / (X[:,0].max() - X[:,0].min())\n",
    "X[:, 1] = (X[:, 1] - X[:,1].min()) / (X[:,1].max() - X[:,1].min())\n",
    "X[:, 2] = (X[:, 2] - X[:,2].min()) / (X[:,2].max() - X[:,2].min())\n",
    "\n",
    "# Seperate traning and test\n",
    "x_trains = X[:320]\n",
    "y_trains = Y[:320]\n",
    "x_tests = X[320:]\n",
    "y_tests = Y[320:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.8153973751\n",
      "-5.24845521747\n",
      "-6.99920865778\n",
      "-6.5542964532\n",
      "-7.70286704142\n",
      "-7.55302431819\n",
      "-4.94112805638\n",
      "-7.2512931868\n",
      "-5.79261287619\n",
      "-4.74860951103\n",
      "Evaluate: 0.7125\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self, eta=0.01):\n",
    "        self.w = np.random.rand(3+1)\n",
    "        self.eta = eta\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.array([self.phi(x) for x in X])\n",
    "\n",
    "    def phi(self, x):\n",
    "        return self.sigmoid(self.w[1:].dot(x)) #+ self.w[0])\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-x))\n",
    "    \n",
    "    def dsigmoid(self, y):\n",
    "        return y * (1 - y)\n",
    "\n",
    "    def likelihood(self, Z, Y):\n",
    "        return np.prod(Z**Y * (1.-Z)**(1-Y))\n",
    "    \n",
    "    def log_likelihood(self, Z, Y):\n",
    "        return np.sum(Y * np.log(Z) + (1 - Y) * np.log(1 - Z))\n",
    "    \n",
    "    def cost(self, Z, Y):\n",
    "        return np.sum(-Y * np.log(Z) - (1 - Y) * np.log(1 - Z))\n",
    "    \n",
    "    def train(self, X, Y, episode_n=10):\n",
    "        for _ in xrange(episode_n):\n",
    "            \n",
    "            for i in range(10000):\n",
    "                sp_x, sp_y = self.sample(X, Y, n=10)\n",
    "                yhat = self.predict(sp_x)\n",
    "                self.gradient_ascent(yhat, sp_x, sp_y)\n",
    "                \n",
    "                if i%1000 == 0:\n",
    "                    X, Y = self.shuffle(X, Y)\n",
    "                \n",
    "                if i == 0:\n",
    "                    print self.log_likelihood(yhat, sp_y)\n",
    "            \n",
    "            \n",
    "    def shuffle(self, X, Y):\n",
    "        rands = np.random.permutation(len(X))\n",
    "        return X[rands], Y[rands]\n",
    "    \n",
    "    def sample(self, X, Y, n=1):\n",
    "        N = len(X)\n",
    "        rands = np.random.randint(0, N, size=n)\n",
    "        return X[rands], Y[rands]\n",
    "    \n",
    "    def gradient_ascent(self, yhat, x, y):\n",
    "        errors = y - yhat\n",
    "        delta = x.T.dot(errors)\n",
    "        self.w[1:] = self.w[1:] + self.eta * delta\n",
    "#         self.w[0] = self.eta * delta.sum()\n",
    "        \n",
    "    def quantizer(self, yhat):\n",
    "        # This is just a unit step function\n",
    "        if yhat >= 0.5:\n",
    "            return 1\n",
    "        return 0\n",
    "                \n",
    "    def evaluate(self, X, Y):\n",
    "        N = len(X)\n",
    "        correct_n = 0\n",
    "        for i in xrange(N):\n",
    "            yhat = self.quantizer(self.predict([X[i]]))\n",
    "            if Y[i] == yhat:\n",
    "                correct_n += 1.\n",
    "        return correct_n/N\n",
    "        \n",
    "        \n",
    "lr = LogisticRegression()\n",
    "lr.train(x_trains, y_trains)\n",
    "print 'Evaluate:', lr.evaluate(x_tests, y_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just calculate likelihood & log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "instance = LogisticRegression()\n",
    "predicted_y = instance.predict(X)\n",
    "\n",
    "plot_x = np.arange(len(Y))\n",
    "xlim(-10, len(Y) + 10)\n",
    "ylim(0, 1)\n",
    "title(u'predicted result with randomly-generated weights')\n",
    "ylabel('predicted')\n",
    "scatter(plot_x[Y == 0], predicted_y[Y == 0], color='#555555', label='y=0')\n",
    "scatter(plot_x[Y == 1], predicted_y[Y == 1], color='red', label='y=1')\n",
    "legend()\n",
    "\n",
    "print u'likelihood의 경우는 numeric underflow가 심하게 나타납니다.'\n",
    "print '-'*60\n",
    "print 'likelihood:\\t', instance.likelihood(predicted_y, Y)\n",
    "print 'log-likelihood:\\t', instance.log_likelihood(predicted_y, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate cost \n",
    "\n",
    "cost functiond은 log-likelihood를 변형하여 (마이너스 붙여주기) gradient descent를 사용할수 있도록 해줍니다.<br>\n",
    "하나의 샘플을 갖고서 cost를 구하면 다음과 같습니다.\n",
    "\n",
    "만약 $ y = 1 $ 이라면.. <br>\n",
    "$ J(\\phi(z), y; \\mathbf{w}) = -\\ln \\left( \\phi(z) \\right) $\n",
    "\n",
    "만약 $ y = 0 $ 이라면.. <br>\n",
    "$ J(\\phi(z), y; \\mathbf{w}) = -\\ln \\left( 1 - \\phi(z) \\right) $\n",
    "\n",
    "아래 plot을 보면.. y값과 예측값이 맞아 떨어질수록 cost값 또한 동일하게 줄어드는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def y_equal_1(Z):\n",
    "    return -np.log(Z)\n",
    "\n",
    "def y_equal_0(Z):\n",
    "    return -np.log(1.-Z)\n",
    "\n",
    "_Z = np.arange(0, 1, 0.01)\n",
    "\n",
    "grid()\n",
    "plot(_Z, y_equal_0(_Z), color='#AA3939', label='y=0')\n",
    "plot(_Z, y_equal_1(_Z), color='#008379', label='y=1')\n",
    "legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
