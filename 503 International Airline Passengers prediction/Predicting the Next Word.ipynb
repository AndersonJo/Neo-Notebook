{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import rnn_with_numpy as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anderson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_START = '_SETENCE_START'\n",
    "SENTENCE_END = '_SENTENCE_END'\n",
    "UNKNOWN_TOKEN = '_UNKNOWN_TOKEN'\n",
    "VOCAB_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Done\n",
      "Finished:  0.563012\n"
     ]
    }
   ],
   "source": [
    "with open('/dataset/reddit_comments/reddit_comments_small.csv', 'rt') as f:\n",
    "    t = datetime.now()\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    \n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(\n",
    "        *[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    print('Split Done')\n",
    "    \n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (SENTENCE_START, x, SENTENCE_END)\n",
    "                    for x in sentences ]\n",
    "    print('Finished: ', (datetime.now() - t).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(st) for st in sentences ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17196 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"%d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Word Indexing\n",
    "\n",
    "아래 2개의 함수를 사용해서 word -> index 또는 그 반대로 변환을 해 줄수 있습니다.\n",
    "\n",
    "* index_to_word(index) : list -> word\n",
    "* word_to_index(word) : dict -> integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 8000\n",
      "Unknown token Index: 7999\n",
      "The least frequent word in our vocabulary is 'indicating' and appeared 1 times.\n"
     ]
    }
   ],
   "source": [
    "most_common_freq = word_freq.most_common(VOCAB_SIZE-1)\n",
    "index_to_word = [x[0] for x in most_common_freq]\n",
    "index_to_word.append(UNKNOWN_TOKEN)\n",
    "word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "\n",
    "print(\"VOCAB_SIZE: %d\" % VOCAB_SIZE)\n",
    "print(\"Unknown token Index:\", word_to_index[UNKNOWN_TOKEN])\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % \\\n",
    "          (most_common_freq[-1][0], most_common_freq[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Make Unknown Tokens\n",
    "\n",
    "링크, 이상한 단어들.. 등등. 모든 단어들을 모두 외우고 있을수는 없습니다.<br>\n",
    "따라서 가장 많이 나온 단어들 (word_freq.most_common(VOCAB_SIZE-1)) 을 제외하고, 그 외 단어들은 UNKNOWN_TOKEN으로 대체합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 67, 510, 68, 35, 91, 20, 7266, 32, 12, 3006, 34]\n",
      "[67, 510, 68, 35, 91, 20, 7266, 32, 12, 3006, 34, 1]\n"
     ]
    }
   ],
   "source": [
    "# Create the training data\n",
    "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "print(x_train[1])\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = rnn.RNNNumpy(VOCAB_SIZE)\n",
    "o, s = model.forward_propagation(x_train[9])\n",
    "p = model.predict(x_train[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error cost: 8.9873874381\n"
     ]
    }
   ],
   "source": [
    "print('error cost:', model.cross_entropy(x_train[26], y_train[26]))\n",
    "model.bptt(x_train[26], y_train[26])\n",
    "model.calculate_gradients(x_train[26], y_train[26])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Total Data:  11587\n"
     ]
    }
   ],
   "source": [
    "model.train(x_train, y_train, npoch=len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_idx_to_sentence(index):\n",
    "    return ' '.join([index_to_word[i] for i in index])\n",
    "\n",
    "def convert_sentence_to_idx(sentence):\n",
    "    return  [ word_to_index[w] for w in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result1: [   7 7999 7999 7999 7999 7999 7999 7999]\n",
      "_SETENCE_START here in germany i know kik as\n",
      "i _UNKNOWN_TOKEN _UNKNOWN_TOKEN _UNKNOWN_TOKEN _UNKNOWN_TOKEN _UNKNOWN_TOKEN _UNKNOWN_TOKEN _UNKNOWN_TOKEN\n"
     ]
    }
   ],
   "source": [
    "test1 = convert_sentence_to_idx(SENTENCE_START+ \" here in germany i know kik as\")\n",
    "result1 = model.predict(test1)\n",
    "print('result1:', result1)\n",
    "print(convert_idx_to_sentence(test1))\n",
    "print(convert_idx_to_sentence(result1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
