{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Corpora and Vector Spaces\n",
    "\n",
    "* https://radimrehurek.com/gensim/tut1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim import matutils\n",
    "from pprint import pprint as pp\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# From Strings to Vectors\n",
    "\n",
    "documents안에 들어있는 각각의 element를 **corpus**라고 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize \n",
    "\n",
    "문장을 단어별로 만듭니다. (NLTK 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'for', 'lab', 'abc', 'computer', 'applications'],\n",
      " ['a', 'survey', 'of', 'user', 'opinion', 'of', 'computer', 'system', 'response', 'time'],\n",
      " ['the', 'eps', 'user', 'interface', 'management', 'system'],\n",
      " ['system', 'and', 'human', 'system', 'engineering', 'testing', 'of', 'eps'],\n",
      " ['relation', 'of', 'user', 'perceived', 'response', 'time', 'to', 'error', 'measurement'],\n",
      " ['the', 'generation', 'of', 'random', 'binary', 'unordered', 'trees'],\n",
      " ['the', 'intersection', 'graph', 'of', 'paths', 'in', 'trees'],\n",
      " ['graph', 'minors', 'iv', 'widths', 'of', 'trees', 'and', 'well', 'quasi', 'ordering'],\n",
      " ['graph', 'minors', 'a', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "lowered_documents = list(map(lambda w: w.lower(), documents)) # Lower \n",
    "tokenizers = [nltk.word_tokenize(sentence) for sentence in lowered_documents] # Tokenize\n",
    "        \n",
    "pp(tokenizers, width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Word to Integer ID\n",
    "\n",
    "단어별 tokenize를 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 8,\n",
      " 'abc': 5,\n",
      " 'and': 19,\n",
      " 'applications': 7,\n",
      " 'binary': 29,\n",
      " 'computer': 6,\n",
      " 'engineering': 20,\n",
      " 'eps': 17,\n",
      " 'error': 25,\n",
      " 'for': 3,\n",
      " 'generation': 27,\n",
      " 'graph': 33,\n",
      " 'human': 0,\n",
      " 'in': 35,\n",
      " 'interface': 2,\n",
      " 'intersection': 32,\n",
      " 'iv': 37,\n",
      " 'lab': 4,\n",
      " 'machine': 1,\n",
      " 'management': 18,\n",
      " 'measurement': 26,\n",
      " 'minors': 36,\n",
      " 'of': 10,\n",
      " 'opinion': 12,\n",
      " 'ordering': 41,\n",
      " 'paths': 34,\n",
      " 'perceived': 23,\n",
      " 'quasi': 40,\n",
      " 'random': 28,\n",
      " 'relation': 22,\n",
      " 'response': 14,\n",
      " 'survey': 9,\n",
      " 'system': 13,\n",
      " 'testing': 21,\n",
      " 'the': 16,\n",
      " 'time': 15,\n",
      " 'to': 24,\n",
      " 'trees': 31,\n",
      " 'unordered': 30,\n",
      " 'user': 11,\n",
      " 'well': 39,\n",
      " 'widths': 38}\n"
     ]
    }
   ],
   "source": [
    "word2vec = corpora.Dictionary(tokenizers)\n",
    "pp(word2vec.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Back of Word\n",
    "\n",
    "document를 back of word 로 변환시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(6, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)],\n",
      " [(2, 1), (11, 1), (13, 1), (16, 1), (17, 1), (18, 1)],\n",
      " [(0, 1), (10, 1), (13, 2), (17, 1), (19, 1), (20, 1), (21, 1)],\n",
      " [(10, 1), (11, 1), (14, 1), (15, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)],\n",
      " [(10, 1), (16, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)],\n",
      " [(10, 1), (16, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1)],\n",
      " [(10, 1), (19, 1), (31, 1), (33, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)],\n",
      " [(8, 1), (9, 1), (33, 1), (36, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [word2vec.doc2bow(token) for token in tokenizers]\n",
    "pp(corpus, width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Corpus to Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix shape: (42, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  2.,  0.,  1.,  1.,  1.,  1.,  1.,  0.],\n",
       "       [ 0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = matutils.corpus2dense(corpus, 42)\n",
    "print('matrix shape:', matrix.shape)\n",
    "matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
