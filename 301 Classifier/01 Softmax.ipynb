{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier\n",
    "\n",
    "* http://cs231n.github.io/linear-classify/#softmax 참고 \n",
    "* https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "\n",
    "\n",
    "Softmax function은 N-dimensional vector를 받아서 다시 N-dimensional vector로 return을 합니다. <br>\n",
    "이때 output은 0~1 사이의 확률분포를 갖으며, 전체의 합은 1이 됩니다. $ S(a) : \\mathbb{R}^{\\mathbb{N}} \\rightarrow \\mathbb{R}^{\\mathbb{N}} $\n",
    "\n",
    "각각의 element단위의 공식은 다음과 같습니다. \n",
    "\n",
    "$$ S_i = \\frac{e^{x_i}}{\\sum^N_{k=1} e^{x_k}} $$\n",
    "\n",
    "$ S_i $ 는 exponent연산을 하기 때문에 항상 positive 값을 갖습니다.<br>\n",
    "또한 numerator부분이 denominator에서 합쳐져서 나오기 때문에  0~1 사이의 확률분포로 값이 나오게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "아래 예제에서 `[1.0, 2.0, 3.0]` 은 softmax함수에 의해서 `[0.09, 0.24, 0.67]` 로 transform되어야 하며, 합은 1이 되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value    : [1, 2, 3]\n",
      "softmax  : [ 0.09003057  0.24472847  0.66524096]\n",
      "summed up: 1.0\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x/np.sum(exp_x, axis=0)\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = softmax(a)\n",
    "\n",
    "print('value    :', a)\n",
    "print('softmax  :', b)\n",
    "print('summed up:', np.sum(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "* [Udacity - Softmax](https://classroom.udacity.com/courses/ud730/lessons/6370362152/concepts/63815621490923#)\n",
    "\n",
    "1번정답 <br>\n",
    "```\n",
    "[ 0.09003057  0.24472847  0.66524096]\n",
    "```\n",
    "\n",
    "2번정답<br>\n",
    "```\n",
    "[[ 0.09003057  0.00242826  0.01587624  0.33333333]\n",
    " [ 0.24472847  0.01794253  0.11731043  0.33333333]\n",
    " [ 0.66524096  0.97962921  0.86681333  0.33333333]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09003057  0.00242826  0.01587624  0.33333333]\n",
      " [ 0.24472847  0.01794253  0.11731043  0.33333333]\n",
      " [ 0.66524096  0.97962921  0.86681333  0.33333333]] 4.0\n"
     ]
    }
   ],
   "source": [
    "scores1 = np.array([1.0, 2.0, 3.0])\n",
    "scores2 = np.array([[1, 2, 3, 6],\n",
    "                    [2, 4, 5, 6], \n",
    "                    [3, 8, 7, 6]])\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x/np.sum(exp_x, axis=0)\n",
    "\n",
    "# plot(scores1, softmax(scores1))\n",
    "print(softmax(scores2), np.sum(softmax(scores2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
